<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="../css/strict.xsl"?>
<PAGE xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../css/schema.xsd" xml:lang="en">
<TITLE>PyTorch</TITLE>
<PATH>notes/pytorch.xml</PATH>
<DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>10</DAY></DATE>
<CONTENT>
<DEFINITIONTABLE>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.tensor.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>constructs a tensor with no autograd history (a.k.a. a "leaf tensor") by copying data.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>Tensor.shape</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.Tensor.shape.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns the size of the <CODEROUTINE>self</CODEROUTINE> tensor.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>Tensor.size(dim=None) → torch.Size or int</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.Tensor.size.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns the size of the <CODEROUTINE>self</CODEROUTINE> tensor. If <CODEROUTINE>dim</CODEROUTINE> is not specified, the returned value is a <CODEROUTINE>torch.Size</CODEROUTINE>, a subclass of <CODEROUTINE>tuple</CODEROUTINE>. If <CODEROUTINE>dim</CODEROUTINE> is specified, returns an int holding the size of that dimension.</DESC>
  </ROW>
</DEFINITIONTABLE>
<BR/>
<DEFINITIONTABLE>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.arange(start=0, end, step=1, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.arange.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns a 1-D tensor of size <MATH>\left\lceil \frac{\text{end} - \text{start}}{\text{step}} \right\rceil</MATH> with values from the interval [start, end) taken with common difference step beginning from start.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.rand.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)[0,1) The shape of the tensor is defined by the variable argument <CODEROUTINE>size</CODEROUTINE>.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.randint(low=0, high, size, \*, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.randint.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns a tensor filled with random integers generated uniformly between <CODEROUTINE>low</CODEROUTINE> (inclusive) and <CODEROUTINE>high</CODEROUTINE> (exclusive). The shape of the tensor is defined by the variable argument <CODEROUTINE>size</CODEROUTINE>.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.multinomial(input, num_samples, replacement=False, *, generator=None, out=None) → LongTensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.multinomial.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns a tensor where each row contains <CODEROUTINE>num_samples</CODEROUTINE> indices sampled from the multinomial probability distribution located in the corresponding row of tensor <CODEROUTINE>input</CODEROUTINE>.</DESC>
  </ROW>
</DEFINITIONTABLE>
<BR/>
<DEFINITIONTABLE>
  <ROW>
    <TERM><X><T><CODEROUTINE>Tensor.view(*shape) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.Tensor.view.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns a new tensor with the same data as the <CODEROUTINE>self</CODEROUTINE> tensor but of a different shape.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.cat(tensors, dim=0, *, out=None) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.cat.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>concatenates the given sequence of <CODEROUTINE>seq</CODEROUTINE> tensors in the given dimension. All tensors must either have the same shape (except in the concatenating dimension) or be empty.
      <CODESAMPLE>
        <PROMPT/>t1 = torch.tensor([[1, 2, 3], [4, 5, 6]])<BR/>
        <PROMPT/>t2 = torch.tensor([[11, 12, 13], [14, 15, 16], [17, 18, 19]])<BR/>
        <PROMPT/>t3 = torch.cat((t1, t2))<BR/>
        <PROMPT/>print(t1, t2, t3, sep='\n')<BR/>
        tensor([[1,&#xA0;2,&#xA0;3],<BR/>
        &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;[4,&#xA0;5,&#xA0;6]])<BR/>
        tensor([[11,&#xA0;12,&#xA0;13],<BR/>
        &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;[14,&#xA0;15,&#xA0;16],<BR/>
        &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;[17,&#xA0;18,&#xA0;19]])<BR/>
        tensor([[&#xA0;1,&#xA0;&#xA0;2,&#xA0;&#xA0;3],<BR/>
        &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;[&#xA0;4,&#xA0;&#xA0;5,&#xA0;&#xA0;6],<BR/>
        &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;[11,&#xA0;12,&#xA0;13],<BR/>
        &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;[14,&#xA0;15,&#xA0;16],<BR/>
        &#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;&#xA0;[17,&#xA0;18,&#xA0;19]])</CODESAMPLE></DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.tril(input, diagonal=0, *, out=None) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.tril.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns the lower triangular part of the matrix (2-D tensor) or batch of matrices <CODEROUTINE>input</CODEROUTINE>, the other elements of the result tensor <CODEROUTINE>out</CODEROUTINE> are set to 0.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.triu(input, diagonal=0, *, out=None) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.triu.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns the upper triangular part of the matrix (2-D tensor) or batch of matrices <CODEROUTINE>input</CODEROUTINE>, the other elements of the result tensor <CODEROUTINE>out</CODEROUTINE> are set to 0.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.exp(input, *, out=None) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.exp.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns a new tensor with the exponential of the elements of the input tensor <CODEROUTINE>input</CODEROUTINE>.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.nn.Softmax(dim=None)</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>applies the Softmax function to an n-dimensional input <CODEROUTINE>Tensor</CODEROUTINE> rescaling them so that the elements of the n-dimensional output <CODEROUTINE>Tensor</CODEROUTINE> lie in the range [0,1] and sum to 1.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.transpose(input, dim0, dim1) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.transpose.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>returns a tensor that is a transposed version of <CODEROUTINE>input</CODEROUTINE>. The given dimensions <CODEROUTINE>dim0</CODEROUTINE> and <CODEROUTINE>dim1</CODEROUTINE> are swapped.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.stack(tensors, dim=0, *, out=None) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.stack.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>concatenates a sequence of tensors along a new dimension. All tensors need to be of the same size.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.mm(input, mat2, *, out=None) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.mm.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>performs a matrix multiplication of the matrices <CODEROUTINE>input</CODEROUTINE> and <CODEROUTINE>mat2</CODEROUTINE>.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.matmul(input, other, *, out=None) → Tensor</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.matmul.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>performs a matrix product of two tensors. The behavior depends on the dimensionality of the tensors.<BR/>
      The <CODEROUTINE>@</CODEROUTINE> operand can also be used to multiply two tensors.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.save.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>saves an object to a disk file.</DESC>
  </ROW>
  <ROW>
    <TERM><X><T><CODEROUTINE>torch.load(f, map_location=None, pickle_module=pickle, *, weights_only=False, mmap=None, **pickle_load_args)</CODEROUTINE></T><A>https://pytorch.org/docs/stable/generated/torch.load.html</A><L>en</L><F>HTML</F></X></TERM>
    <DESC>loads an object saved with <CODEROUTINE>torch.save()</CODEROUTINE> from a file.</DESC>
  </ROW>
</DEFINITIONTABLE>
<BR/>
The optimizers are listed <X><T>here</T><A>https://pytorch.org/docs/stable/optim.html</A><L>en</L><F>HTML</F></X>.<BR/>
</CONTENT>
<X><T>links</T><A>../links/ai.html#pytorch</A><L>en</L><F>HTML</F></X>
</PAGE>