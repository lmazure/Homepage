<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="../css/strict.xsl"?>
<PAGE xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../css/schema.xsd" xml:lang="en">
<TITLE>ML implementation</TITLE>
<PATH>links/aiimplementation.xml</PATH>
<DATE><YEAR>2024</YEAR><MONTH>9</MONTH><DAY>1</DAY></DATE>
<CONTENT>
<LLIST>
  <ITEM><SLIST>
    <ITEM><BLIST><TITLE>Machine learning</TITLE>
      <ITEM><BLIST><TITLE><X><T>OpenAI</T><A>https://openai.com/</A><L>en</L><F>HTML</F></X></TITLE>
        <ITEM><X><T>doc</T><A>https://platform.openai.com/docs/overview</A><L>en</L><F>HTML</F></X></ITEM>
      </BLIST></ITEM>
      <ITEM><BLIST><TITLE><X><T>Anthropic</T><A>https://www.anthropic.com/</A><L>en</L><F>HTML</F></X></TITLE>
        <ITEM><X><T>courses</T><A>https://github.com/anthropics/courses</A><L>en</L><F>HTML</F></X></ITEM>
      </BLIST></ITEM>
      <ITEM><X><T>Mistral AI</T><A>https://mistral.ai</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>TensorFlow</T><A>https://www.tensorflow.org</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>Hugging Face</T><A>https://huggingface.co/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>Kaggle</T><A>https://www.kaggle.com/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>BigML</T><A>https://bigml.com</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>SentenceTransformers</T><A>https://www.sbert.net/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>Lightning AI’s Deep Learning Fundamentals</T><A>https://lightning.ai/courses/deep-learning-fundamentals/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><BLIST><TITLE><ANCHOR>pytorch</ANCHOR><X><T>Pytorch</T><A>https://pytorch.org/</A><L>en</L><F>HTML</F></X></TITLE>
        <ITEM><X><T>Tutorial</T><A>https://pytorch.org/tutorials/</A><L>en</L><F>HTML</F></X></ITEM>
        <ITEM><X><T>personal notes</T><A>../notes/pytorch.html</A><L>en</L><F>HTML</F></X></ITEM>
      </BLIST></ITEM>
      <ITEM><X><T>Machine Learning for Software Engineering</T><A>https://github.com/saltudelft/ml4se</A><L>en</L><F>HTML</F></X>: a curated list of papers, PhD theses, datasets, and tools</ITEM>
    </BLIST></ITEM>
    <ITEM><BLIST><TITLE>Optimisation</TITLE>
      <ITEM><X><T>Méthode de Nelder-Mead</T><A>https://fr.wikipedia.org/wiki/M%C3%A9thode_de_Nelder-Mead</A><L>fr</L><F>HTML</F></X></ITEM>
    </BLIST></ITEM>
    <ITEM><BLIST><TITLE>Jupyter notebooks</TITLE>
      <ITEM><X><T>Google Colab</T><A>https://colab.google/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>Kaggle</T><A>https://www.kaggle.com/code</A><L>en</L><F>HTML</F></X></ITEM>
    </BLIST></ITEM>
    <ITEM><CLIST><TITLE><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR></TITLE>
      <ITEM><X><T>Homepage</T><A>https://karpathy.ai/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X><T>Blog</T><A>https://karpathy.github.io/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X quality="2"><T>YouTube</T><A>https://www.youtube.com/channel/UCXUPKJO5MZQN11PqgIvyuvQ</A><L>en</L><F>HTML</F><FEED><A>https://www.youtube.com/feeds/videos.xml?channel_id=UCXUPKJO5MZQN11PqgIvyuvQ</A><F>Atom</F></FEED></X></ITEM>
    </CLIST></ITEM>
    <ITEM><CLIST><TITLE><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR></TITLE>
      <ITEM><X><T>YouTube</T><A>https://www.youtube.com/channel/UCZHmQk67mSJgfCCTn7xBfew</A><L>en</L><F>HTML</F><FEED><A>https://www.youtube.com/feeds/videos.xml?channel_id=UCZHmQk67mSJgfCCTn7xBfew</A><F>Atom</F></FEED></X></ITEM>
    </CLIST></ITEM>
    <ITEM><CLIST><TITLE><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR></TITLE>
      <ITEM><X><T>Homepage</T><A>https://sebastianraschka.com/</A><L>en</L><F>HTML</F></X></ITEM>
      <ITEM><X quality="1"><T>Ahead of AI</T><A>https://magazine.sebastianraschka.com/</A><L>en</L><F>HTML</F><FEED><A>https://magazine.sebastianraschka.com/feed</A><F>Atom</F></FEED></X></ITEM>
    </CLIST></ITEM>
    <ITEM><CLIST><TITLE>AI Coffee Break with Letitia (<AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR>)</TITLE>
      <ITEM><X><T>YouTube</T><A>https://www.youtube.com/channel/UCobqgqE4i5Kf7wrxRxhToQA</A><L>en</L><F>HTML</F><FEED><A>https://www.youtube.com/feeds/videos.xml?channel_id=UCobqgqE4i5Kf7wrxRxhToQA</A><F>Atom</F></FEED></X></ITEM>
      <ITEM><X><T>Substack</T><A>https://aicoffeebreakwl.substack.com/</A><L>en</L><F>HTML</F></X></ITEM>
    </CLIST></ITEM>
  </SLIST></ITEM>
  <ITEM><BLIST><TITLE>Articles and videos</TITLE>
    <ITEM><ARTICLE><X quality="1"><T>SDS 771: Gradient Boosting: XGBoost, LightGBM and CatBoost, with Kirill Eremenko</T><A>https://www.superdatascience.com/podcast/gradient-boosting-xgboost-lightgbm-and-catboost-with-kirill-eremenko</A><L>en</L><F>MP3</F><DURATION><HOUR>1</HOUR><MINUTE>55</MINUTE><SECOND>27</SECOND></DURATION></X><X quality="1"><T>771: Gradient Boosting: XGBoost, LightGBM and CatBoost — with Kirill Eremenko</T><A>https://www.youtube.com/watch?v=tqR8jGm6Zhw</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>57</MINUTE><SECOND>31</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Kirill</FIRSTNAME><LASTNAME>Eremenko</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Jon</FIRSTNAME><LASTNAME>Krohn</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>2</DAY></DATE><COMMENT>A good basic presentation of some algorithm based on decision trees: XGBoost, LightGBM, and CatBoost.</COMMENT></ARTICLE></ITEM>
    <ITEM><BLIST><TITLE>Neural networks</TITLE>
      <ITEM><ARTICLE><X><T>Polyworld: Using Evolution to Design Artificial Intelligence</T><A>https://www.youtube.com/watch?v=_m97_kL4ox0</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>6</MINUTE><SECOND>39</SECOND></DURATION><DATE><YEAR>2007</YEAR><MONTH>11</MONTH><DAY>13</DAY></DATE></X><AUTHOR><FIRSTNAME>Virgil</FIRSTNAME><LASTNAME>Griffith</LASTNAME></AUTHOR><DATE><YEAR>2007</YEAR><MONTH>11</MONTH><DAY>8</DAY></DATE><COMMENT>Using artificial life to optimise neural networks.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Evolving Neural Networks to Play 2048</T><A>https://www.youtube.com/watch?v=jsVnuw5Bv0s</A><L>en</L><F>MP4</F><DURATION><MINUTE>5</MINUTE><SECOND>43</SECOND></DURATION></X><AUTHOR><FIRSTNAME>John</FIRSTNAME><LASTNAME>Downey</LASTNAME></AUTHOR><DATE><YEAR>2014</YEAR><MONTH>5</MONTH><DAY>11</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Is Dr. Calvin in the Room?</T><A>https://blog.cleancoder.com/uncle-bob/2017/03/16/DrCalvin.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><MIDDLENAME>Cecil</MIDDLENAME><LASTNAME>Martin</LASTNAME><GIVENNAME>Uncle Bob</GIVENNAME></AUTHOR><DATE><YEAR>2017</YEAR><MONTH>3</MONTH><DAY>16</DAY></DATE><COMMENT>Some ideas and some dubious simple back-of-the-envelope calculations about neural networks.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>How to generate text: using different decoding methods for language generation with Transformers</T><A>https://huggingface.co/blog/how-to-generate</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Patrick</FIRSTNAME><LASTNAME>von Platen</LASTNAME></AUTHOR><DATE><YEAR>2020</YEAR><MONTH>3</MONTH><DAY>1</DAY></DATE><COMMENT>A presentation of the different methods to control the text generated by a model: greedy search, beam search, sampling, top-K sampling, and top-p (nucleus) sampling.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>The Neural Network, A Visual Introduction</T><A>https://www.youtube.com/watch?v=UOvPeC8WOt8</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>51</SECOND></DURATION></X><AUTHOR><GIVENNAME>vcubingx</GIVENNAME></AUTHOR><DATE><YEAR>2020</YEAR><MONTH>8</MONTH><DAY>23</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Gradients are Not All You Need (Machine Learning Research Paper Explained)</T><A>https://www.youtube.com/watch?v=EeMhj0sPrhE</A><L>en</L><F>MP4</F><DURATION><MINUTE>48</MINUTE><SECOND>29</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2021</YEAR><MONTH>11</MONTH><DAY>15</DAY></DATE><COMMENT>A paper ("<X><T>Gradients are Not All You Need</T><A>https://arxiv.org/abs/2111.05803</A><L>en</L><F>HTML</F></X>") showing that gradient backpropagation does not work properly for some chaotic systems.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Machine Learning 1: Tour d'horizon et le cas MuZero (feat Dalle2, PaLM) - Passe-science #47</T><A>https://www.youtube.com/watch?v=Yoq5cfndzhM</A><L>fr</L><F>MP4</F><DURATION><MINUTE>30</MINUTE><SECOND>4</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Thomas</FIRSTNAME><LASTNAME>Cabaret</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>6</MONTH><DAY>4</DAY></DATE><COMMENT>The latest results of the best AIs and how Muzero is trained.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE predecessor="https://www.youtube.com/watch?v=Yoq5cfndzhM"><X><T>Machine Learning 2: Architecture et Alphastar (Transformer, attention) - Passe-science #48</T><A>https://www.youtube.com/watch?v=SzHCGniWr-0</A><L>fr</L><F>MP4</F><DURATION><MINUTE>29</MINUTE><SECOND>29</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Thomas</FIRSTNAME><LASTNAME>Cabaret</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>6</MONTH><DAY>11</DAY></DATE><COMMENT>A description of the architecture of Alphastart and of transformers.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>TensorFlow in 100 Seconds</T><A>https://www.youtube.com/watch?v=i8NETqtGHms</A><L>en</L><F>MP4</F><DURATION><MINUTE>2</MINUTE><SECOND>38</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Jeff</FIRSTNAME><LASTNAME>Delaney</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>8</MONTH><DAY>3</DAY></DATE><COMMENT>A very short example of using TensorFlow.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="1"><T>The spelled-out intro to neural networks and backpropagation: building micrograd</T><A>https://www.youtube.com/watch?v=VMj-3S1tku0</A><L>en</L><F>MP4</F><DURATION><HOUR>2</HOUR><MINUTE>25</MINUTE><SECOND>51</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>8</MONTH><DAY>16</DAY></DATE><COMMENT>A good basic introduction to backpropagation with the code details.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Understanding Encoder And Decoder LLMs</T><A>https://magazine.sebastianraschka.com/p/understanding-encoder-and-decoder</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>6</MONTH><DAY>17</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="-1"><T>Create a Large Language Model from Scratch with Python – Tutorial</T><A>https://www.youtube.com/watch?v=UU1WVnMk4E8</A><L>en</L><F>MP4</F><DURATION><HOUR>5</HOUR><MINUTE>43</MINUTE><SECOND>40</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Elliot</FIRSTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>8</MONTH><DAY>25</DAY></DATE><COMMENT>This lengthy tutorial is not worth watching. Many parts lack preparation, some explanations are confusing, <AUTHOR><FIRSTNAME>Elliot</FIRSTNAME></AUTHOR> spends a significant amount of time explaining simple concepts while skipping complex ones… We get some understanding on how to implement an LLM, but this would easily be accomplished in a one-hour video.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>LLM Training: RLHF and Its Alternatives</T><A>https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>9</MONTH><DAY>10</DAY></DATE><COMMENT>As said in the title, a clear description of RHLF and its alternatives.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>What is LoRA? Low-Rank Adaptation for finetuning LLMs EXPLAINED</T><A>https://www.youtube.com/watch?v=KEv-F5UkhxU</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>21</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>9</MONTH><DAY>18</DAY></DATE><COMMENT>A presentation of LoRA.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Practical Tips for Finetuning LLMs Using LoRA (Low-Rank Adaptation)</T><ST>Things I Learned From Hundreds of Experiments</ST><A>https://magazine.sebastianraschka.com/p/practical-tips-for-finetuning-llms</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>19</DAY></DATE><COMMENT>Some experiments with LoRA.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>What is Q-Learning (back to basics)</T><A>https://www.youtube.com/watch?v=nOBm4aYEYR4</A><L>en</L><F>MP4</F><DURATION><MINUTE>45</MINUTE><SECOND>43</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>25</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Mixture of Experts Explained</T><A>https://huggingface.co/blog/moe</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Omar</FIRSTNAME><LASTNAME>Sanseviero</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lewis</FIRSTNAME><LASTNAME>Tunstall</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Philipp</FIRSTNAME><LASTNAME>Schmid</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Sourab</FIRSTNAME><LASTNAME>Mangrulkar</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Younes</FIRSTNAME><LASTNAME>Belkada</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Pedro</FIRSTNAME><LASTNAME>Cuenca</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>11</DAY></DATE><COMMENT>A technical history of the MoE models.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Apple is doing the UNTHINKABLE!!!</T><A>https://www.youtube.com/watch?v=_EUejKPutBc</A><L>en</L><F>MP4</F><DURATION><MINUTE>7</MINUTE><SECOND>20</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>6</DAY></DATE><COMMENT>Some information about Apple’s MLX Framework.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>LLaMA Pro: Progressive LLaMA with Block Expansion (Paper Explained)</T><A>https://www.youtube.com/watch?v=hW3OVWfndLw</A><L>en</L><F>MP4</F><DURATION><MINUTE>31</MINUTE><SECOND>45</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>7</DAY></DATE><COMMENT>Commenting a paper ("<X><T>LLaMA Pro: Progressive LLaMA with Block Expansion</T><A>https://arxiv.org/abs/2401.02415</A><L>en</L><F>HTML</F></X>") which contains dubious claims about how the researchers improved LLaMA by duplicating some blocks.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>A Guide to Deeplearning4j</T><A>https://www.baeldung.com/deeplearning4j</A><L>en</L><F>HTML</F></X><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>8</DAY></DATE><COMMENT>A short presentation of Deeplearning4j.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="1"><T>Sampling for Text Generation</T><A>https://huyenchip.com/2024/01/16/sampling.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Chip</FIRSTNAME><LASTNAME>Huyen</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>16</DAY></DATE><COMMENT>A clear overview on the methods used to sample or constrain the output of a generative AI.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>AlphaGeometry: Solving olympiad geometry without human demonstrations (Paper Explained)</T><A>https://www.youtube.com/watch?v=ZNK4nfgNQpM</A><L>en</L><F>MP4</F><DURATION><MINUTE>35</MINUTE><SECOND>26</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>21</DAY></DATE><COMMENT>A summary of the <X><T>paper</T><A>https://www.nature.com/articles/s41586-023-06747-5</A><L>en</L><F>HTML</F></X> describing AlphaGeometry.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>LLMs itself CAN create BETTER LLMs</T><A>https://www.youtube.com/watch?v=12jdFZrh8j4</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>14</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>22</DAY></DATE><COMMENT>A quick presentation of "<X><T>Self-Rewarding Language Models</T><A>https://arxiv.org/abs/2401.10020</A><L>en</L><F>HTML</F></X>": a dubious claim that a model can be improved by training and rewarding itself a new iteration of itself.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Model Merging, Mixtures of Experts, and Towards Smaller LLMs</T><A>https://magazine.sebastianraschka.com/p/research-papers-in-january-2024</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>3</DAY></DATE><COMMENT>Weight Averaged Reward Models, Tuning Language Models by Proxy, Mixtral of Experts, and TinyLlama.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Sparse LLMs at inference: 6x faster transformers! | DEJAVU paper explained</T><A>https://www.youtube.com/watch?v=DUkWMoi5nG4</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>16</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>3</DAY></DATE><COMMENT>A presentation of "<X><T>Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time</T><A>https://arxiv.org/abs/2310.17157</A><L>en</L><F>HTML</F></X>", some self-attention heads and some MLP neurons are selected by running a simpler neuron network.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Lumiere: A Space-Time Diffusion Model for Video Generation (Paper Explained)</T><A>https://www.youtube.com/watch?v=Pl8BET_K1mc</A><L>en</L><F>MP4</F><DURATION><MINUTE>54</MINUTE><SECOND>23</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>4</DAY></DATE><COMMENT>An opinionated presentation of "<X><T>Lumiere: A Space-Time Diffusion Model for Video Generation</T><A>https://arxiv.org/abs/2401.12945</A><L>en</L><F>HTML</F></X>".</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>"MORE AGENTS" Is All You Need</T><A>https://www.youtube.com/watch?v=AAv6NlNRMIQ</A><L>en</L><F>MP4</F><DURATION><MINUTE>17</MINUTE><SECOND>53</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>12</DAY></DATE><COMMENT>"<X><T>More Agents Is All You Need</T><A>https://arxiv.org/abs/2402.05120</A><L>en</L><F>HTML</F></X>" analyses the gain of generating answers from several LLMs and using a voting mechanism to define the final answer.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>How Quickly Do Large Language Models Learn Unexpected Skills?</T><ST>A new study suggests that so-called emergent abilities actually develop gradually and predictably, depending on how you measure them.</ST><A>https://www.quantamagazine.org/how-quickly-do-large-language-models-learn-unexpected-skills-20240213/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Stephen</FIRSTNAME><LASTNAME>Ornes</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>13</DAY></DATE><COMMENT>The debate about emergent capabilities appearing abruptly or continuously is still going on…</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="1"><T>Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch</T><A>https://magazine.sebastianraschka.com/p/lora-and-dora-from-scratch</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>18</DAY></DATE><COMMENT>A clear description of LoRA and DoRA.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="2"><T>Let's build the GPT Tokenizer</T><A>https://www.youtube.com/watch?v=zduSFxRajkE</A><L>en</L><F>MP4</F><DURATION><HOUR>2</HOUR><MINUTE>13</MINUTE><SECOND>34</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>20</DAY></DATE><COMMENT>A very good description of tokenisation.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="-1"><T>Floating Points are no more, Changes everything for LLMs!!!</T><A>https://www.youtube.com/watch?v=Gtf3CxIRiPk</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>18</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>28</DAY></DATE><COMMENT>"<X><T>The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits</T><A>https://arxiv.org/abs/2402.17764</A><L>en</L><F>HTML</F></X>" describes an LLM using only -1, 0, and 1 as weights.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="-1"><T>How Selective Forgetting Can Help AI Learn Better</T><ST>Erasing key information during training results in machine learning models that can learn new languages faster and more easily.</ST><A>https://www.quantamagazine.org/how-selective-forgetting-can-help-ai-learn-better-20240228/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Amos</FIRSTNAME><LASTNAME>Zeeberg</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>28</DAY></DATE><COMMENT>Some little and too basic information about forgetting language models which are easier to train to new languages.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Research Papers in February 2024: A LoRA Successor, Small Finetuned LLMs Vs Generalist LLMs, and Transparent LLM Research</T><A>https://magazine.sebastianraschka.com/p/research-papers-in-february-2024</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>3</DAY></DATE><COMMENT>Can small fined-tuned models perform better on some tasks than large models, DoRA, OLMo is a real open-source model, Gemma…</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Tips for LLM Pretraining and Evaluating Reward Models</T><ST>Discussing AI Research Papers in March 2024</ST><A>https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>31</DAY></DATE><COMMENT>An analysis of continuous pretraining and a benchmark for evaluating reward models.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>How do mixture-of-experts layers affect transformer models?</T><ST>This new LLM technique has started improving the results of models without additional training.</ST><A>https://stackoverflow.blog/2024/04/04/how-do-mixture-of-experts-layers-affect-transformer-models/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Cameron</FIRSTNAME><MIDDLENAME>R.</MIDDLENAME><LASTNAME>Wolfe</LASTNAME><NAMESUFFIX>PhD</NAMESUFFIX></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>4</DAY></DATE><COMMENT>A short description of the Mixture of Experts architecture, I guess that if you know enough to understand this, you already know about MoE.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping (Searchformer)</T><A>https://www.youtube.com/watch?v=PW4JiJ-WaY4</A><L>en</L><F>MP4</F><DURATION><MINUTE>44</MINUTE><SECOND>4</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>6</DAY></DATE><COMMENT>"<X><T>Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping</T><A>https://arxiv.org/abs/2402.14083</A><L>en</L><F>HTML</F></X>": using a transformer model to mimic an A<SUP>*</SUP> search algorithm, results are better when the model also has to reproduce the search traces.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Why Recurrent Neural Networks are cursed | LM2</T><A>https://www.youtube.com/watch?v=rTz6hadM1Lg</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>16</SECOND></DURATION></X><AUTHOR><GIVENNAME>vcubingx</GIVENNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>8</DAY></DATE><COMMENT>A presentation of Recurrent Neural Networks.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Flow Matching for Generative Modeling (Paper Explained)</T><A>https://www.youtube.com/watch?v=7NNxK3CqaDk</A><L>en</L><F>MP4</F><DURATION><MINUTE>56</MINUTE><SECOND>15</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>8</DAY></DATE><COMMENT>"<X><T>Flow Matching for Generative Modeling</T><A>https://arxiv.org/abs/2210.02747</A><L>en</L><F>HTML</F></X>": a mathematical description of Flow Matching, a mechanism to train Continuous Normalizing Flows.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>How did the Attention Mechanism start an AI frenzy? | LM3</T><A>https://www.youtube.com/watch?v=lOrTlKrdmkQ</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>54</SECOND></DURATION></X><AUTHOR><GIVENNAME>vcubingx</GIVENNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>15</DAY></DATE><COMMENT>How the attention mechanism was implemented for RNN.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</T><A>https://www.youtube.com/watch?v=r_UBBfTPcF0</A><L>en</L><F>MP4</F><DURATION><MINUTE>37</MINUTE><SECOND>16</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>24</DAY></DATE><COMMENT>A paper ("<X><T>Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention</T><A>https://arxiv.org/abs/2404.07143</A><L>en</L><F>HTML</F></X>") describing how to support infinite context length, but <AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR> has some doubts about its real effectiveness.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>How AI 'Understands' Images (CLIP) - Computerphile</T><A>https://www.youtube.com/watch?v=KcSXcpluDe4</A><L>en</L><F>MP4</F><DURATION><MINUTE>18</MINUTE><SECOND>4</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Mike</FIRSTNAME><LASTNAME>Pound</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>25</DAY></DATE><COMMENT>A basic presentation of CLIP.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>TransformerFAM: Feedback attention is working memory</T><A>https://www.youtube.com/watch?v=3a0_hAiFKag</A><L>en</L><F>MP4</F><DURATION><MINUTE>37</MINUTE><SECOND>0</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>28</DAY></DATE><COMMENT>Yet another paper ("<X><T>TransformerFAM: Feedback attention is working memory</T><A>https://arxiv.org/abs/2404.09173</A><L>en</L><F>HTML</F></X>") proposing infinite context by using additional tokens as a short term memory.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>ORPO: Monolithic Preference Optimization without Reference Model (Paper Explained)</T><A>https://www.youtube.com/watch?v=52kMBrAI_IM</A><L>en</L><F>MP4</F><DURATION><MINUTE>33</MINUTE><SECOND>25</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>1</DAY></DATE><COMMENT>A paper ("<X><T>ORPO: Monolithic Preference Optimization without Reference Model</T><A>https://arxiv.org/abs/2403.07691</A><L>en</L><F>HTML</F></X>") combining supervised fine-tuning and preference alignment.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Shapley Values Explained | Interpretability for AI models, even LLMs!</T><A>https://www.youtube.com/watch?v=5-1lKFvV1i0</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>58</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>6</DAY></DATE><COMMENT>A presentation of Shapley values, a method to explain how much each input impacts the model’s output, and an example with Llama 2 and the SHAP library.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Has Generative AI Already Peaked? - Computerphile</T><A>https://www.youtube.com/watch?v=dDUC-LqVrPU</A><L>en</L><F>MP4</F><DURATION><MINUTE>12</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Mike</FIRSTNAME><LASTNAME>Pound</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>9</DAY></DATE><COMMENT>A basic presentation of a paper ("<X><T>No "Zero-Shot" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance</T><A>https://arxiv.org/abs/2404.04125</A><L>en</L><F>HTML</F></X>") claiming that multimodal models require exponentially more data to achieve linear improvements.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>GaLore EXPLAINED: Memory-Efficient LLM Training by Gradient Low-Rank Projection</T><A>https://www.youtube.com/watch?v=VC9NbOir7q0</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>37</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>27</DAY></DATE><COMMENT>Yet another pre-training/fine-tuning algorithm: "<X><T>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection</T><A>https://arxiv.org/abs/2403.03507</A><L>en</L><F>HTML</F></X>".</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>LLM Research Insights: Instruction Masking and New LoRA Finetuning Experiments</T><ST>Discussing the Latest Model Releases and AI Research in May 2024</ST><A>https://magazine.sebastianraschka.com/p/llm-research-insights-instruction</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>2</DAY></DATE><COMMENT>Three papers: not masking instructions when calculating the loss for instruction finetuning performs better than masking, but only if the answer is sort and the number of training examples is small; LoRA learns less and forgets less than full finetuning; MoRa is yet another finetuning algorithm.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Scalable MatMul-free Language Modeling (Paper Explained)</T><A>https://www.youtube.com/watch?v=B45FlSQ8ITo</A><L>en</L><F>MP4</F><DURATION><MINUTE>49</MINUTE><SECOND>45</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>7</MONTH><DAY>8</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR> presents and comments "<X><T>Scalable MatMul-free Language Modeling</T><A>https://arxiv.org/abs/2406.02528</A><L>en</L><F>HTML</F></X>", a paper proposing to replace matrix multiplication with ternary weights.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="-1"><T>Machine Learning and Logistic Regression</T><A>https://www.youtube.com/watch?v=AX-ZEC-71DI</A><L>en</L><F>MP4</F><DURATION><MINUTE>5</MINUTE><SECOND>12</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Diarra</FIRSTNAME><LASTNAME>Bell</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>7</MONTH><DAY>19</DAY></DATE><COMMENT>A bad description of logistic regression, the linear part is not explained.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X quality="-1"><T>A New Type of Neural Network Is More Interpretable</T><ST>Kolmogorov-Arnold Networks could point physicists to new hypotheses</ST><A>https://spectrum.ieee.org/kan-neural-network</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Matthew</FIRSTNAME><LASTNAME>Hutson</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>5</DAY></DATE><COMMENT>There is little valuable information about KAN networks in this article.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Reinforcement Learning from Human Feedback (RLHF) Explained</T><A>https://www.youtube.com/watch?v=T_X4XFwKX8k</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>28</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Martin</FIRSTNAME><LASTNAME>Keen</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>7</DAY></DATE><COMMENT>The title says it all: a short presentation of RLHF.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>New LLM Pre-training and Post-training Paradigms</T><ST>A Look at How Modern LLMs Are Trained</ST><A>https://magazine.sebastianraschka.com/p/new-llm-pre-training-and-post-training</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>17</DAY></DATE><COMMENT>An overview and comparison of the pre and post-trainings of Qwen 2, Apple Foundation Model, Gemma 2, and Llama 3.1.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution – Paper Explained</T><A>https://www.youtube.com/watch?v=K_9wQ6LZNpI</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>21</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>20</DAY></DATE><COMMENT>"<X><T>Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution</T><A>https://arxiv.org/abs/2310.16834</A><L>en</L><F>HTML</F></X>" describes a diffusion model algorithm for text that gives correct results, before such models used to generate garbage text.</COMMENT></ARTICLE></ITEM>
      <ITEM><BLIST><TITLE>Generative Adversarial Networks</TITLE>
        <ITEM><ARTICLE><X><T>Generative Adversarial Networks (GANs) - Computerphile</T><A>https://www.youtube.com/watch?v=Sw9r8CL98N0</A><L>en</L><F>MP4</F><DURATION><MINUTE>21</MINUTE><SECOND>20</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2017</YEAR><MONTH>10</MONTH><DAY>25</DAY></DATE><COMMENT>Adversarial Networks and using them to generate images.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Zebras, Horses &amp; CycleGAN - Computerphile</T><A>https://www.youtube.com/watch?v=T-lBMrjZ3_0</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>10</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Mike</FIRSTNAME><LASTNAME>Pound</LASTNAME></AUTHOR><DATE><YEAR>2019</YEAR><MONTH>8</MONTH><DAY>1</DAY></DATE><COMMENT>A description of CycleGAN, two GANs working in opposite directions.</COMMENT></ARTICLE></ITEM>
      </BLIST></ITEM>
      <ITEM><BLIST><TITLE>Transformers</TITLE>
        <ITEM><ARTICLE><X><T>AI Language Models &amp; Transformers - Computerphile</T><A>https://www.youtube.com/watch?v=rURRYI66E54</A><L>en</L><F>MP4</F><DURATION><MINUTE>20</MINUTE><SECOND>39</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2019</YEAR><MONTH>6</MONTH><DAY>26</DAY></DATE><COMMENT>The usage and implementation of language models, and the new attention-based ones: transformers.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Transformers, explained: Understand the model behind GPT, BERT, and T5</T><A>https://www.youtube.com/watch?v=SZorAJ4I-sA</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>10</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Dale</FIRSTNAME><LASTNAME>Markowitz</LASTNAME></AUTHOR><DATE><YEAR>2021</YEAR><MONTH>8</MONTH><DAY>18</DAY></DATE><COMMENT>A basic presentation of transformers.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Scaling Transformer to 1M tokens and beyond with RMT (Paper Explained)</T><A>https://www.youtube.com/watch?v=4Cclp6yPDuw</A><L>en</L><F>MP4</F><DURATION><MINUTE>24</MINUTE><SECOND>33</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>27</DAY></DATE><COMMENT>An explanation of the "<X><T>Scaling Transformer to 1M tokens and beyond with RMT</T><A>https://arxiv.org/abs/2304.11062</A><L>en</L><F>HTML</F></X>" paper: a <KEYWORD><KEYID>Recursive neural network</KEYID><KEYEDTEXT>RNN</KEYEDTEXT></KEYWORD> where the base building block is a transformer.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Understanding and Coding Self-Attention, Multi-Head Attention, Cross-Attention, and Causal-Attention in LLMs</T><A>https://magazine.sebastianraschka.com/p/understanding-and-coding-self-attention</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>14</DAY></DATE><COMMENT>A clear and detailed description of self-attention implementation.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Transformers explained | The architecture behind LLMs</T><A>https://www.youtube.com/watch?v=ec9IQMiJBhs</A><L>en</L><F>MP4</F><DURATION><MINUTE>19</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>21</DAY></DATE><COMMENT>Yet another explanation of the transformer architecture. This one is correct.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>SDS 759: Full Encoder-Decoder Transformers Fully Explained, with Kirill Eremenko</T><A>https://www.superdatascience.com/podcast/full-encoder-decoder-transformers-fully-explained-with-kirill-eremenko</A><L>en</L><F>MP3</F><DURATION><HOUR>1</HOUR><MINUTE>43</MINUTE><SECOND>13</SECOND></DURATION></X><X><T>759: Full Encoder-Decoder Transformers Fully Explained — with Kirill Eremenko</T><A>https://www.youtube.com/watch?v=6ue7W5DhWbs</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>40</MINUTE><SECOND>27</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Kirill</FIRSTNAME><LASTNAME>Eremenko</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Jon</FIRSTNAME><LASTNAME>Krohn</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>20</DAY></DATE><COMMENT>Yet another presentation of transformers, this one is so-so.</COMMENT></ARTICLE></ITEM>
      </BLIST></ITEM>
      <ITEM><BLIST><TITLE>MAMBA</TITLE>
        <ITEM><ARTICLE><X><T>Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Paper Explained)</T><A>https://www.youtube.com/watch?v=9dSkvxS2EB0</A><L>en</L><F>MP4</F><DURATION><MINUTE>40</MINUTE><SECOND>40</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>24</DAY></DATE><COMMENT>The paper presenting the Mamba architecture: "<X><T>Mamba: Linear-Time Sequence Modeling with Selective State Spaces</T><A>https://arxiv.org/abs/2312.00752</A><L>en</L><F>HTML</F></X>".</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>This CRAZY Paper on Mamba has got some REAL Juice!!!</T><A>https://www.youtube.com/watch?v=i23Zz4fsfp0</A><L>en</L><F>MP4</F><DURATION><MINUTE>14</MINUTE><SECOND>51</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>6</DAY></DATE><COMMENT>A shallow presentation of the "<X><T>Repeat After Me: Transformers are Better than State Space Models at Copying</T><A>https://arxiv.org/abs/2402.01032</A><L>en</L><F>HTML</F></X>" paper.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>MAMBA and State Space Models explained | SSM explained</T><A>https://www.youtube.com/watch?v=vrF3MtGwD0Y</A><L>en</L><F>MP4</F><DURATION><MINUTE>22</MINUTE><SECOND>26</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>17</DAY></DATE><COMMENT>Another presentation of MAMBA.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>The FIRST Production-grade Mamba-based LLM!!!</T><A>https://www.youtube.com/watch?v=Pd0_GKykmdE</A><L>en</L><F>MP4</F><DURATION><MINUTE>10</MINUTE><SECOND>1</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>31</DAY></DATE><COMMENT>A presentation of <X><T>ai21labs/Jamba-v0.1</T><A>https://huggingface.co/ai21labs/Jamba-v0.1</A><L>en</L><F>HTML</F></X>, a hybrid Mamba/Transformer architecture.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Attention!!! JAMBA Instruct - Mamba LLM's new Baby!!!</T><A>https://www.youtube.com/watch?v=ZZ7prpxjKhE</A><L>en</L><F>MP4</F><DURATION><MINUTE>6</MINUTE><SECOND>38</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>2</DAY></DATE><COMMENT>Jamba-Instruct, a chatbot based on Jamba.</COMMENT></ARTICLE></ITEM>
      </BLIST></ITEM>
      <ITEM><BLIST><TITLE>xLSTM</TITLE>
        <ITEM><ARTICLE><X><T>xLSTM: Extended Long Short-Term Memory</T><A>https://www.youtube.com/watch?v=0OaEv1a5jUM</A><L>en</L><F>MP4</F><DURATION><MINUTE>56</MINUTE><SECOND>59</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>1</DAY></DATE><COMMENT>A presentation of "<X><T>xLSTM: Extended Long Short-Term Memory</T><A>https://arxiv.org/abs/2405.04517</A><L>en</L><F>HTML</F></X>", a study of LSTM variants scalled to billions of parameters.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>xLSTM Explained in Detail!!!</T><A>https://www.youtube.com/watch?v=KuRpxvMMrlk</A><L>en</L><F>MP4</F><DURATION><MINUTE>32</MINUTE><SECOND>52</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Maximilian</FIRSTNAME><LASTNAME>Beck</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>7</MONTH><DAY>1</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Maximilian</FIRSTNAME><LASTNAME>Beck</LASTNAME></AUTHOR>, an author of the previous paper, is presenting xLSTM.</COMMENT></ARTICLE></ITEM>
      </BLIST></ITEM>
      <ITEM><BLIST><TITLE>PyTorch</TITLE>
        <ITEM><ARTICLE><X><T>Débuter avec PyTorch</T><A>https://ledatascientist.com/debuter-avec-pytorch/</A><L>fr</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Salma</FIRSTNAME><LASTNAME>Elghourbal</LASTNAME></AUTHOR><DATE><YEAR>2021</YEAR><MONTH>3</MONTH><DAY>18</DAY></DATE><COMMENT>A short introduction to Pytorch with a small example.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Building a Single Layer Neural Network in PyTorch</T><A>https://machinelearningmastery.com/building-a-single-layer-neural-network-in-pytorch/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Muhammad Asad</FIRSTNAME><LASTNAME>Iqbal Khan</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>8</DAY></DATE><COMMENT>A complete and very simple example.</COMMENT></ARTICLE></ITEM>
        <ITEM><BLIST><TITLE><X><T>Get SH*T Done with PyTorch</T><A>https://github.com/curiousily/Getting-Things-Done-with-Pytorch</A><L>en</L><F>HTML</F></X></TITLE>
          <ITEM><ARTICLE><X><T>Getting Started with PyTorch</T><A>https://curiousily.com/posts/getting-started-with-pytorch/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Venelin</FIRSTNAME><LASTNAME>Valkov</LASTNAME></AUTHOR><DATE><YEAR>2020</YEAR><MONTH>2</MONTH><DAY>6</DAY></DATE><COMMENT>A short introduction to PyTorch.</COMMENT></ARTICLE></ITEM>
          <ITEM><ARTICLE><X><T>Build Your First Neural Network with PyTorch</T><A>https://curiousily.com/posts/build-your-first-neural-network-with-pytorch/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Venelin</FIRSTNAME><LASTNAME>Valkov</LASTNAME></AUTHOR><DATE><YEAR>2020</YEAR><MONTH>2</MONTH><DAY>21</DAY></DATE><COMMENT>A simple neural network.</COMMENT></ARTICLE></ITEM>
        </BLIST></ITEM>
      </BLIST></ITEM>
      <ITEM><BLIST><TITLE>3blue1brown’s Deep learning</TITLE>
        <ITEM><ARTICLE><X><T>But what is a neural network? | Chapter 1, Deep learning</T><A>https://www.youtube.com/watch?v=aircAruvnKk</A><L>en</L><F>MP4</F><DURATION><MINUTE>18</MINUTE><SECOND>39</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Grant</FIRSTNAME><LASTNAME>Sanderson</LASTNAME></AUTHOR><DATE><YEAR>2017</YEAR><MONTH>10</MONTH><DAY>5</DAY></DATE><COMMENT>A basic introduction to the structure of neutral networks and how such a structure could work.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Gradient descent, how neural networks learn | Chapter 2, Deep learning</T><A>https://www.youtube.com/watch?v=IHZwWFHWa-w</A><L>en</L><F>MP4</F><DURATION><MINUTE>20</MINUTE><SECOND>33</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Grant</FIRSTNAME><LASTNAME>Sanderson</LASTNAME></AUTHOR><DATE><YEAR>2017</YEAR><MONTH>10</MONTH><DAY>16</DAY></DATE><COMMENT>Training a neural network consists in minimising a cost function and how to use gradient descent to perform this minimisation.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>What is backpropagation really doing? | Chapter 3, Deep learning</T><A>https://www.youtube.com/watch?v=Ilg3gGewQ5U</A><L>en</L><F>MP4</F><DURATION><MINUTE>12</MINUTE><SECOND>46</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Grant</FIRSTNAME><LASTNAME>Sanderson</LASTNAME></AUTHOR><DATE><YEAR>2017</YEAR><MONTH>11</MONTH><DAY>3</DAY></DATE><COMMENT>Getting a feeling of how backpropagation works.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Backpropagation calculus | Chapter 4, Deep learning</T><A>https://www.youtube.com/watch?v=tIeHLnjs5U8</A><L>en</L><F>MP4</F><DURATION><MINUTE>10</MINUTE><SECOND>17</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Grant</FIRSTNAME><LASTNAME>Sanderson</LASTNAME></AUTHOR><DATE><YEAR>2017</YEAR><MONTH>11</MONTH><DAY>3</DAY></DATE><COMMENT>The calculus expressions of backpropagation.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>But what is a GPT?  Visual intro to transformers | Chapter 5, Deep Learning</T><A>https://www.youtube.com/watch?v=wjZofJX0v4M</A><L>en</L><F>MP4</F><DURATION><MINUTE>27</MINUTE><SECOND>13</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Grant</FIRSTNAME><LASTNAME>Sanderson</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>1</DAY></DATE><COMMENT>A very high-level description of an LLM architecture and the description of embedding and softmax.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>Attention in transformers, visually explained | Chapter 6, Deep Learning</T><A>https://www.youtube.com/watch?v=eMlx5fFNoYc</A><L>en</L><F>MP4</F><DURATION><MINUTE>26</MINUTE><SECOND>9</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Grant</FIRSTNAME><LASTNAME>Sanderson</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>7</DAY></DATE><COMMENT>A description of the transformer architecture.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X quality="1"><T>How might LLMs store facts | Chapter 7, Deep Learning</T><A>https://www.youtube.com/watch?v=9-Jl0dxWQs8</A><L>en</L><F>MP4</F><DURATION><MINUTE>22</MINUTE><SECOND>42</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Grant</FIRSTNAME><LASTNAME>Sanderson</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>31</DAY></DATE><COMMENT>A description of how the multilayer perceptrons complete embedded vectors with related data during inference.</COMMENT></ARTICLE></ITEM>
      </BLIST></ITEM>
      <ITEM><BLIST><TITLE>Grokking</TITLE>
        <ITEM><ARTICLE><X><T>Grokking: Generalization beyond Overfitting on small algorithmic datasets (Paper Explained)</T><A>https://www.youtube.com/watch?v=dND-7llwrpw</A><L>en</L><F>MP4</F><DURATION><MINUTE>29</MINUTE><SECOND>46</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2021</YEAR><MONTH>10</MONTH><DAY>6</DAY></DATE><COMMENT>A paper ("<X><T>Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets</T><A>https://arxiv.org/abs/2201.02177</A><L>en</L><F>HTML</F></X>") on the grokking phenomenon where generalisation seems to happen abruptly and long after fitting the training data.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>"Grokking" : les modèles d'IA sont-ils capables de piger ?</T><ST>Ce phénomène étonnant, découvert récemment, pourrait changer notre compréhension de l'apprentissage et de la cognition dans les réseaux de neurones...</ST><A>https://scienceetonnante.substack.com/p/grokking-les-modeles-dia-sont-ils</A><L>fr</L><F>HTML</F></X><AUTHOR><FIRSTNAME>David</FIRSTNAME><LASTNAME>Louapre</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>9</MONTH><DAY>11</DAY></DATE><COMMENT>Another presentation of grokking.</COMMENT></ARTICLE></ITEM>
        <ITEM><ARTICLE><X><T>How Do Machines ‘Grok’ Data?</T><ST>By apparently overtraining them, researchers have seen neural networks discover novel solutions to problems.</ST><A>https://www.quantamagazine.org/how-do-machines-grok-data-20240412/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Anil</FIRSTNAME><LASTNAME>Ananthaswamy</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>12</DAY></DATE><COMMENT>Some researchers understood some cases of grokking, but the phenomenon has only been studied from small neural network doing modular arithmetic.</COMMENT></ARTICLE></ITEM>
      </BLIST></ITEM>
    </BLIST></ITEM>
  </BLIST></ITEM>
</LLIST>
</CONTENT>
</PAGE>