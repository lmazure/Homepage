<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="../css/strict.xsl"?>
<PAGE xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../css/schema.xsd" xml:lang="en">
<TITLE>Large language models</TITLE>
<PATH>links/llm.xml</PATH>
<DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>30</DAY></DATE>
<CONTENT>
<LLIST>
  <ITEM><BLIST><TITLE>Articles and videos</TITLE>
    <ITEM><ARTICLE><X><T>Inside a radical new project to democratize AI</T><ST>A group of over 1,000 AI researchers has created a multilingual large language model bigger than GPT-3—and they’re giving it out for free.</ST><A>https://www.technologyreview.com/2022/07/12/1055817/inside-a-radical-new-project-to-democratize-ai/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Melissa</FIRSTNAME><LASTNAME>Heikkilä</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>7</MONTH><DAY>12</DAY></DATE><COMMENT>The description of the BLOOM project: building a free LLM.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Ch(e)at GPT? - Computerphile</T><A>https://www.youtube.com/watch?v=XZJc1p6RE78</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>51</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Mike</FIRSTNAME><LASTNAME>Pound</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>2</MONTH><DAY>16</DAY></DATE><COMMENT>Some researchers propose a hidden statistical signature for text generated by a large language model.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>LLaMA: Open and Efficient Foundation Language Models (Paper Explained)</T><A>https://www.youtube.com/watch?v=E5OnoYF2oAk</A><L>en</L><F>MP4</F><DURATION><MINUTE>41</MINUTE><SECOND>6</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>2</DAY></DATE><COMMENT>Some comments on a LLaMA paper.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>Glitch Tokens - Computerphile</T><A>https://www.youtube.com/watch?v=WO2X3oZEJOA</A><L>en</L><F>MP4</F><DURATION><MINUTE>19</MINUTE><SECOND>28</SECOND></DURATION><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>7</DAY></DATE></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>3</DAY></DATE><COMMENT>The problem of meaningless tokens learned by LLMs and resulting in crazy answers.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Emergent Abilities of Large Language Models</T><ST>Emergence can be defined as the sudden appearance of novel behavior. Large Language Models apparently display emergence by suddenly gaining new abilities as they grow. Why does this happen, and what does this mean?</ST><A>https://www.assemblyai.com/blog/emergent-abilities-of-large-language-models</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Ryan</FIRSTNAME><LASTNAME>O’Connor</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>7</DAY></DATE><COMMENT>The emergent capabilities of LLM as they get larger and two possible explanations.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Baidu shares fall after Ernie AI chatbot demo disappoints</T><ST>After demo, no one knows if Ernie can compete with ChatGPT.</ST><A>https://arstechnica.com/information-technology/2023/03/chinese-search-giant-launches-ai-chatbot-with-prerecorded-demo/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Ryan</FIRSTNAME><LASTNAME>McMorrow</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Qianer</FIRSTNAME><LASTNAME>Liu</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>16</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>What's Up With Bard? 9 Examples + 6 Reasons Google Fell Behind [ft. Muse, Med-PaLM 2 and more]</T><A>https://www.youtube.com/watch?v=XmnTd92NqFw</A><L>en</L><F>MP4</F><DURATION><MINUTE>12</MINUTE><SECOND>8</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Philip</FIRSTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>22</DAY></DATE><COMMENT>A comparison of Bard and GPT-4 and so hypotheses why Bard is bad.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Brace Yourself for a Tidal Wave of ChatGPT Email Scams</T><ST>Thanks to large language models, a single scammer can run hundreds or thousands of cons in parallel, night and day, in every language under the sun.</ST><A>https://www.wired.com/story/large-language-model-phishing-scams/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Barath</FIRSTNAME><LASTNAME>Raghavan</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>3</DAY></DATE><COMMENT>The authors claim that AI will help scammers because it will be possible to easily deal with many potential victims in parallel, but will these scams be really effective?</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>ChatGPT vs Google Bard: Which is better? We put them to the test.</T><ST>We compare two top AI language models in seven categories to pick a winner.</ST><A>https://arstechnica.com/information-technology/2023/04/clash-of-the-ai-titans-chatgpt-vs-bard-in-a-showdown-of-wits-and-wisdom/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>5</DAY></DATE><COMMENT>How to easily write an article.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>Why ChatGPT and Bing Chat are so good at making things up</T><ST>A look inside the hallucinating artificial minds of the famous text prediction bots.</ST><A>https://arstechnica.com/information-technology/2023/04/why-ai-chatbots-are-the-ultimate-bs-machines-and-how-people-hope-to-fix-them/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>6</DAY></DATE><COMMENT>A good basic explanation of how Chat LLMs work.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>China slaps security reviews on AI products as Alibaba unveils ChatGPT challenger</T><ST>Regulator warns AI-created content should embody "socialist values."</ST><A>https://arstechnica.com/information-technology/2023/04/china-slaps-security-reviews-on-ai-products-as-alibaba-unveils-chatgpt-challenger/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Ryan</FIRSTNAME><LASTNAME>McMorrow</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Nian</FIRSTNAME><LASTNAME>Liu</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>11</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>The mounting human and environmental costs of generative AI</T><ST>Op-ed: Planetary impacts, escalating financial costs, and labor exploitation all factor.</ST><A>https://arstechnica.com/gadgets/2023/04/generative-ai-is-cool-but-lets-not-forget-its-human-and-environmental-costs/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sasha</FIRSTNAME><LASTNAME>Luccioni</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>12</DAY></DATE><COMMENT>Some problems with LLMs. There is nothing new here, but this is still a good overview.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>“A really big deal”—Dolly is a free, open source, ChatGPT-style AI model</T><ST>Dolly 2.0 could spark a new wave of fully open source LLMs similar to ChatGPT.</ST><A>https://arstechnica.com/information-technology/2023/04/a-really-big-deal-dolly-is-a-free-open-source-chatgpt-style-ai-model/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>13</DAY></DATE><COMMENT>Databricks released Dolly 2.0, an open source LLM that can be used even in commercial products.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Stability AI launches StableLM, an open source ChatGPT alternative</T><ST>StableLM's 3B and 7B models are available now on GitHub under CC 4.0 license.</ST><A>https://arstechnica.com/information-technology/2023/04/stable-diffusion-for-language-stability-launches-open-source-ai-chatbot/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>24</DAY></DATE><COMMENT>Yet another open source LLM.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Understanding Parameter-Efficient LLM Finetuning: Prompt Tuning And Prefix Tuning</T><A>https://magazine.sebastianraschka.com/p/understanding-parameter-efficient</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>30</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Exploring ChatGPT vs open-source models on slightly harder tasks</T><A>https://medium.com/@marcotcr/exploring-chatgpt-vs-open-source-models-on-slightly-harder-tasks-aa0395c31610</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Marco Túlio</FIRSTNAME><LASTNAME>Ribeiro</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Scott</FIRSTNAME><LASTNAME>Lundberg</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>5</MONTH><DAY>12</DAY></DATE><COMMENT>A comparison of ChatGPT 3.5, Vicuna, and MPT.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Big Tech Isn’t Prepared for A.I.’s Next Chapter</T><ST>Open source is changing everything</ST><A>https://slate.com/technology/2023/05/ai-regulation-open-source-meta.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Jim</FIRSTNAME><LASTNAME>Waldo</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>5</MONTH><DAY>30</DAY></DATE><COMMENT>An analysis of the impact of open source LLMs.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-2"><T>Direct Preference Optimization:  Forget RLHF (PPO)</T><A>https://www.youtube.com/watch?v=pzh2oc6shic</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>9</SECOND></DURATION></X><AUTHOR><GIVENNAME>code_your_own_AI</GIVENNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>6</MONTH><DAY>6</DAY></DATE><COMMENT>A description of the paper "<X><T>(Direct Preference Optimization: Your Language Model is Secretly a Reward Model)</T><A>https://arxiv.org/abs/2305.18290</A><L>en</L><F>HTML</F></X>", but the guy does not seem to understand what he is talking about.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>De l'art superflu d'écrire des dissertations à l'heure de ChatGPT</T><A>https://www.youtube.com/watch?v=tzO_uqP1QKk</A><L>fr</L><F>MP4</F><DURATION><MINUTE>12</MINUTE><SECOND>37</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Thibaut</FIRSTNAME><LASTNAME>Giraud</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>6</MONTH><DAY>10</DAY></DATE><COMMENT>Should we teach students to use LLM rather than asking them to still write dissertations.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Sarah Silverman sues OpenAI, Meta for being “industrial-strength plagiarists”</T><ST>AI models allegedly trained on books copied from popular pirate e-book sites.</ST><A>https://arstechnica.com/information-technology/2023/07/book-authors-sue-openai-and-meta-over-text-used-to-train-ai/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Ashley</FIRSTNAME><LASTNAME>Belanger</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>7</MONTH><DAY>10</DAY></DATE><COMMENT>Will the AI companies have to pay back for the data they illegally reaped from Internet?</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Redditors prank AI-powered news mill with “Glorbo” in World of Warcraft</T><ST>"Glorbo" isn't real, but a news-writing AI model didn't know it—and then it wrote about itself.</ST><A>https://arstechnica.com/gaming/2023/07/redditors-prank-ai-powered-news-mill-with-glorbo-in-world-of-warcraft/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>7</MONTH><DAY>21</DAY></DATE><COMMENT>People start to trick news sites which are using AI to automatically generate articles.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>A New Attack Impacts Major AI Chatbots—and No One Knows How to Stop It</T><ST>Researchers found a simple way to make ChatGPT, Bard, and other chatbots misbehave, proving that AI is hard to tame.</ST><A>https://www.wired.com/story/ai-adversarial-attacks/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Will</FIRSTNAME><LASTNAME>Knight</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>8</MONTH><DAY>1</DAY></DATE><COMMENT>Yet another LLM jail breaking.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Can Two AIs Play the TDD Pairing Game?</T><A>https://www.mechanical-orchard.com/insights/can-two-ais-play-the-tdd-pairing-game</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Roberto</FIRSTNAME><LASTNAME>Ostinelli</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>8</MONTH><DAY>16</DAY></DATE><COMMENT>Two AIs practising Ping-pong Programming.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Tiny Language Models Come of Age</T><ST>To better understand how neural networks learn to simulate writing, researchers trained simpler versions on synthetic children’s stories.</ST><A>https://www.quantamagazine.org/tiny-language-models-thrive-with-gpt-4-as-a-teacher-20231005/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Ben</FIRSTNAME><LASTNAME>Brubaker</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>10</MONTH><DAY>5</DAY></DATE><COMMENT>Some Microsoft researchers trained "small" models with child stories generated by GPT4, these models are able to generate stories.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution (Paper Explained)</T><A>https://www.youtube.com/watch?v=tkX0EfNl4Fc</A><L>en</L><F>MP4</F><DURATION><MINUTE>46</MINUTE><SECOND>44</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>10</MONTH><DAY>7</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR> is not convinced by "<X><T>Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution</T><A>https://arxiv.org/abs/2309.16797</A><L>en</L><F>HTML</F></X>", an experiment using an evolutionary algorithm to find better prompts.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Avoiding LLM hallucinations through analytical AI, is it possible?</T><A>https://golem.ai/en/blog/ia-generative-analytique-neurosymbolique</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Martin</FIRSTNAME><LASTNAME>Deramecourt</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>10</MONTH><DAY>25</DAY></DATE><COMMENT>The experience of a company evaluating the use an LLM to answer to customer queries.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://golem.ai/en/blog/ia-generative-analytique-neurosymbolique"><X><T>LLM Performance Optimization with Nvidia GPUs from Scaleway: A Technical Study</T><A>https://golem.ai/en/blog/optimisation-llm-scaleway</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Kevin</FIRSTNAME><LASTNAME>Baude</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>10</MONTH><DAY>27</DAY></DATE><COMMENT>Some information on running Llama-2 70B model using llama.cpp.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>This is EXACTLY HOW some LLMs RANK TOP!!!</T><A>https://www.youtube.com/watch?v=lM8gGgiVAnQ</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>22</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>9</DAY></DATE><COMMENT>A paper "<X><T>Don't Make Your LLM an Evaluation Benchmark Cheater</T><A>https://arxiv.org/abs/2311.01964</A><L>en</L><F>PDF</F></X>" states the obvious: leaking benchmark data in the training data will result in better benchmark scores.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>[1hr Talk] Intro to Large Language Models</T><A>https://www.youtube.com/watch?v=zjkBMFhNj_g</A><L>en</L><F>MP4</F><DURATION><MINUTE>59</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>23</DAY></DATE><COMMENT>A good introduction and overview of LLMs.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>"trust me", Google Bard REALLY launched a killer feature!!!</T><A>https://www.youtube.com/watch?v=rinyW5a_dWs</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>23</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>24</DAY></DATE><COMMENT>Bard is now able to get information from YouTube captions.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Extracting Training Data from ChatGPT</T><A>https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Milad</FIRSTNAME><LASTNAME>Nasr</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Nicholas</FIRSTNAME><LASTNAME>Carlini</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Jon</FIRSTNAME><LASTNAME>Hayase</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Matthew</FIRSTNAME><LASTNAME>Jagielski</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>A.</FIRSTNAME><LASTNAME>Feder Cooper</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Daphne</FIRSTNAME><LASTNAME>Ippolito</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Christopher</FIRSTNAME><MIDDLENAME>A.</MIDDLENAME><LASTNAME>Choquette-Choo</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Eric</FIRSTNAME><LASTNAME>Wallace</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Florian</FIRSTNAME><LASTNAME>Tramèr</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Katherine</FIRSTNAME><LASTNAME>Lee</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>28</DAY></DATE><COMMENT>A summary of a research paper ("<X><T>Scalable Extraction of Training Data from (Production) Language Models</T><A>https://arxiv.org/abs/2311.17035</A><L>en</L><F>HTML</F></X>") studying training data extraction attacks and a basic explanation of patching an exploit vs. fixing a vulnerability.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://not-just-memorization.github.io/extracting-training-data-from-chatgpt.html"><X><T>Scalable Extraction of Training Data from (Production) Language Models (Paper Explained)</T><A>https://www.youtube.com/watch?v=KwpeuqT69fw</A><L>en</L><F>MP4</F><DURATION><MINUTE>47</MINUTE><SECOND>37</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>3</DAY></DATE><COMMENT>Some comments about the paper.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Round 2: We test the new Gemini-powered Bard against ChatGPT</T><ST>We run the models through seven categories to determine an updated champion.</ST><A>https://arstechnica.com/ai/2023/12/chatgpt-vs-google-bard-round-2-how-does-the-new-gemini-model-fare/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Kyle</FIRSTNAME><LASTNAME>Orland</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>8</DAY></DATE><COMMENT>An informal comparison of the new Bard (powered by Gemini), the old Bard (PaLM), ChatGPT 4, and ChatGPT 3.5.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Phi-2, Imagen-2, Optimus-Gen-2: Small New Models to Change the World?</T><A>https://www.youtube.com/watch?v=nPgs8THgbuI</A><L>en</L><F>MP4</F><DURATION><MINUTE>17</MINUTE><SECOND>50</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Philip</FIRSTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>13</DAY></DATE><COMMENT>Some information about Phi-2 and the problems with MMLU.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>is this brilliance or accuracy leak?</T><A>https://www.youtube.com/watch?v=L5h77yTTAss</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>13</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>15</DAY></DATE><COMMENT>As <AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR> says himself, he is not enough competent to criticize this <X><T>paper (TinyGSM: achieving &gt; 80% on GSM8k with small language models)</T><A>https://arxiv.org/abs/2312.09241</A><L>en</L><F>HTML</F></X>.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Large Language Models: How Large is Large Enough?</T><A>https://www.youtube.com/watch?v=7a2s3_wkiWo</A><L>en</L><F>MP4</F><DURATION><MINUTE>6</MINUTE><SECOND>52</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Kip</FIRSTNAME><LASTNAME>Yego</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>15</DAY></DATE><COMMENT>A basic comparison of larger and smaller LLMs.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>I tried Eric Hartford's "Save the Kittens" prompt!!!</T><A>https://www.youtube.com/watch?v=-pah-QXnX5E</A><L>en</L><F>MP4</F><DURATION><MINUTE>6</MINUTE><SECOND>59</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>19</DAY></DATE><COMMENT>Some naive prompting…</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Direct Preference Optimization: Your Language Model is Secretly a Reward Model | DPO paper explained</T><A>https://www.youtube.com/watch?v=XZLc09hkMwA</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>54</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>22</DAY></DATE><COMMENT>This description of the differences between DPO and RLHF is not enough detailed to understand how DPO really works.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Open Source LLMs with Simon Willison</T><A>https://podcastaddict.com/oxide-and-friends/episode/169910839</A><L>en</L><F>MP3</F><DURATION><HOUR>1</HOUR><MINUTE>33</MINUTE><SECOND>18</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Bryan</FIRSTNAME><LASTNAME>Cantrill</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Adam</FIRSTNAME><LASTNAME>Leventhal</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>17</DAY></DATE><COMMENT>The current status of LLM, open-weight models, jail breaking, prompt injection…</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>You can get PAID $$$ for Building AI LLMs!!</T><A>https://www.youtube.com/watch?v=YHJi_ri_3DY</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>48</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>31</DAY></DATE><COMMENT>A very unclear description of a reward mechanism for the best fine-tuned models.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI Assistants with OPEN MODELS!!!</T><A>https://www.youtube.com/watch?v=QEv48nXXs1U</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>18</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>2</DAY></DATE><COMMENT>It is now possible to create assistants in HuggingChat.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>This 21B LMM Beats Gemini Pro &amp; GPT-3.5!!! (in Vision)</T><A>https://www.youtube.com/watch?v=wWfMXcHva1k&amp;lc=UgwK4zC2k7zMYXa89FB4AaABAg</A><L>en</L><F>MP4</F><DURATION><MINUTE>15</MINUTE><SECOND>33</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>13</DAY></DATE><COMMENT>A presentation and quick ’n dirty demonstration of Reka Flash.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>The problem with this $50M Funded AI Startup!"</T><A>https://www.youtube.com/watch?v=nG3yHmztq8c</A><L>en</L><F>MP4</F><DURATION><MINUTE>17</MINUTE><SECOND>7</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>2</MONTH><DAY>29</DAY></DATE><COMMENT>Ola’s Kutrim, an Indian LLM, seems not so good…</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI Prompt Engineering Is Dead</T><ST>Long live AI prompt engineering</ST><A>https://spectrum.ieee.org/prompt-engineering-is-dead</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Dina</FIRSTNAME><LASTNAME>Genkina</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>6</DAY></DATE><COMMENT>At last, more people start to explain that "prompt engineering" is bullshit, autotuned prompts or, better, having LLM not requiring tuned prompts is the future.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>22,000 H100s later, Inflection 2.5!!!</T><A>https://www.youtube.com/watch?v=fEpa_Ak6Ec4</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>31</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>7</DAY></DATE><COMMENT>Ye another model claiming to be near GPT4 level.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>The GPT-4 barrier has finally been broken</T><A>https://simonwillison.net/2024/Mar/8/gpt-4-barrier/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>8</DAY></DATE><COMMENT>Some recent models claiming to be on par with GPT4 are arriving: Google Gemini 1.5, Mistral Large, Claude 3 Opus, and Inflection-2.5.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>CANCELED GPT-4 After Talking to Claude 3</T><A>https://www.youtube.com/watch?v=JDlyXJhIMlI</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>17</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>10</DAY></DATE><COMMENT>Is it time to replace using GPT 4 by Claude 3?</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>This NEW LLM "Learnt" to "THINK" BEFORE "TALK"ING!!!</T><A>https://www.youtube.com/watch?v=A_NE3ouBAUI</A><L>en</L><F>MP4</F><DURATION><MINUTE>17</MINUTE><SECOND>7</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>15</DAY></DATE><COMMENT>A presentation of "<X><T>Quiet-STaR: Language Models Can Teach Themselves to Think Before Speaking</T><A>https://arxiv.org/abs/2403.09629</A><L>en</L><F>PDF</F></X>" where the model is trained to generate rationales at each token to explain future text.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Releasing Common Corpus: the largest public domain dataset for training LLMs</T><A>https://huggingface.co/blog/Pclanglais/common-corpus</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Pierre-Carl</FIRSTNAME><LASTNAME>Langlais</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>20</DAY></DATE><COMMENT>The release of Common Corpus, a very large corpus of multilingual and copyright-free texts.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Claude and ChatGPT for ad-hoc sidequests</T><A>https://simonwillison.net/2024/Mar/22/claude-and-chatgpt-case-study/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>22</DAY></DATE><COMMENT>A small example of using Claude 3 Opus and ChatGPT 4.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Inside the Creation of the World’s Most Powerful Open Source AI Model</T><ST>Startup Databricks just released DBRX, the most powerful open source large language model yet—eclipsing Meta’s Llama 2.</ST><A>https://www.wired.com/story/dbrx-inside-the-creation-of-the-worlds-most-powerful-open-source-ai-model/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Will</FIRSTNAME><LASTNAME>Knight</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>27</DAY></DATE><COMMENT>Some basic information about the training of a foundation model.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>A little guide to building Large Language Models in 2024</T><A>https://www.youtube.com/watch?v=2-SPH9hIKT8</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>15</MINUTE><SECOND>52</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Thomas</FIRSTNAME><LASTNAME>Wolf</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>28</DAY></DATE><COMMENT>A good overview of the current technologies used to build an LLM.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>I found this STUNNING Local Perplexity CLONE!!!</T><A>https://www.youtube.com/watch?v=lsp4KhLETTY</A><L>en</L><F>MP4</F><DURATION><MINUTE>12</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>8</DAY></DATE><COMMENT>A presentation of LLocalSearch, a search aggregator using LLMs.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>You can't build a moat with AI</T><ST>It's all about the data</ST><A>https://frontierai.substack.com/p/you-cant-build-a-moat-with-ai</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Vikram</FIRSTNAME><LASTNAME>Sreekanti</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Joseph</FIRSTNAME><MIDDLENAME>E.</MIDDLENAME><LASTNAME>Gonzalez</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>11</DAY></DATE><COMMENT>The value of a system built on top of an LLM is not the model nor the prompt, but the data you provide to the model.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>ChatGPT rêve-t-il de cavaliers électriques ?</T><A>https://www.youtube.com/watch?v=6D1XIbkm4JE</A><L>fr</L><F>MP4</F><DURATION><MINUTE>48</MINUTE><SECOND>23</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Thibaut</FIRSTNAME><LASTNAME>Giraud</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Mathieu</FIRSTNAME><LASTNAME>Acher</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>14</DAY></DATE><COMMENT>It appears that gpt-3.5-turbo-instruct is able to correctly play chess. Some searchers have been able to get smaller LLMs to play Othello and chess, and discovered that the models have built an internal representation of the board.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>How to convert PDF DOCX to Structured TXT Formats for RAG! (UNSTRUCTURED Tutorial)</T><A>https://www.youtube.com/watch?v=iPiYVCl002o</A><L>en</L><F>MP4</F><DURATION><MINUTE>18</MINUTE><SECOND>41</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>16</DAY></DATE><COMMENT>A bad presentation of the unstructured library: a library to extract text from PDF, HTML, Word… documents.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Using and Finetuning Pretrained Transformers</T><A>https://magazine.sebastianraschka.com/p/using-and-finetuning-pretrained-transformers</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>20</DAY></DATE><COMMENT>A list of quickly described options to use and fine-tune a foundation LLM.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Llama 3 from Scratch?? 15T Tokens Data for you!!!</T><A>https://www.youtube.com/watch?v=6QSJ8xc_wG8</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>3</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>22</DAY></DATE><COMMENT>A huge open dataset is available: <X><T>datasets/HuggingFaceFW/fineweb</T><A>https://huggingface.co/datasets/HuggingFaceFW/fineweb</A><L>en</L><F>HTML</F></X>.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>The NEW AI Models ARE A PROBLEM</T><A>https://www.youtube.com/watch?v=AE-YfR511dY</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>6</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>23</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR> is getting tired of the benchmark war. But his discourse is unclear, current LLM are not intelligent, they are only performing some kind of very powerful pattern matching, so we should not expect to get them performing real reasoning, we can only expect them to "remember" and "match" more knowledge.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>New Microsoft AI model may challenge GPT-4 and Google Gemini</T><ST>In project headed by former Inflection chief, MAI-1 may have 500B parameters.</ST><A>https://arstechnica.com/information-technology/2024/05/microsoft-developing-mai-1-language-model-that-may-compete-with-openai-report/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>6</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Mustafa</FIRSTNAME><LASTNAME>Suleyman</LASTNAME></AUTHOR> is leading the create of Microsoft’s own large model.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>How Good Are the Latest Open LLMs? And Is DPO Better Than PPO?</T><ST>Discussing the Latest Model Releases and AI Research in April 2024</ST><A>https://magazine.sebastianraschka.com/p/how-good-are-the-latest-open-llms</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>12</DAY></DATE><COMMENT>A simplistic comparison of Mixtral 8x22B vs. Llama 3 vs. Phi-3, OpenELM, and a comparison of DPO and PPO.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>WARNING: Bad News for CHATGPT!</T><A>https://www.youtube.com/watch?v=ihZXGy-BON8</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>55</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>28</DAY></DATE><COMMENT>A presentation, as bad as usual, of HuggingChat, a chat supporting tools.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Anthropic's Latest Winner - Workbench</T><A>https://www.youtube.com/watch?v=8wD7xeIF3uY</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>13</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Sam</FIRSTNAME><LASTNAME>Witteveen</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>7</MONTH><DAY>10</DAY></DATE><COMMENT>A presentation and a demo of Anthropic Workbench, a tool to generate and evaluate prompts.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Instruction Pretraining LLMs</T><ST>The Latest Research in Instruction Finetuning</ST><A>https://magazine.sebastianraschka.com/p/instruction-pretraining-llms</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>7</MONTH><DAY>20</DAY></DATE><COMMENT>Generating an instruction dataset by providing empty prompts to Llama 3 8B, pretraining models with synthetised data containing raw texts and instruction-response pairs, and some information about Gemma 2.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>[Own work] On Measuring Faithfulness or Self-consistency of Natural Language Explanations</T><A>https://www.youtube.com/watch?v=b3wbTOZXRyI</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>7</MONTH><DAY>26</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR> proposes a self-consistency measurement.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Anthropic's Prompt Engineering Interactive Tutorial</T><A>https://simonwillison.net/2024/Aug/30/anthropic-prompt-engineering-interactive-tutorial/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>30</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR> presents some interesting information nuggets he found in Anthopic documentation.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Mission: Impossible language models – Paper Explained [ACL 2024 recording]</T><A>https://www.youtube.com/watch?v=8lU6dGqR26s</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>5</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>9</MONTH><DAY>2</DAY></DATE><COMMENT>A presentation of a paper ("<X><T>Mission: Impossible Language Models</T><A>https://aclanthology.org/2024.acl-long.787/</A><L>en</L><F>HTML</F></X>") claiming to disprove <AUTHOR><FIRSTNAME>Noam</FIRSTNAME><LASTNAME>Chomsky</LASTNAME></AUTHOR>’s claim that LLMs can learn languages that are possible and impossible for humans to learn.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>I am a Strange Dataset: Metalinguistic Tests for Language Models – Paper Explained [🔴 at ACL 2024]</T><A>https://www.youtube.com/watch?v=m_nEIsQBh_c</A><L>en</L><F>MP4</F><DURATION><MINUTE>4</MINUTE><SECOND>7</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>9</MONTH><DAY>10</DAY></DATE><COMMENT>A short presentation of a dataset containing self-referencing sentences ("<X><T>I am a Strange Dataset: Metalinguistic Tests for Language Models</T><A>https://arxiv.org/abs/2401.05300</A><L>en</L><F>HTML</F></X>"). It appears that LLMs are bad at handling them.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>#452 – Dario Amodei: Anthropic CEO on Claude, AGI &amp; the Future of AI &amp; Humanity</T><A>https://lexfridman.com/dario-amodei</A><L>en</L><F>MP3</F><DURATION><HOUR>5</HOUR><MINUTE>22</MINUTE><SECOND>13</SECOND></DURATION></X><X quality="1"><T>Dario Amodei: Anthropic CEO on Claude, AGI &amp; the Future of AI &amp; Humanity | Lex Fridman Podcast #452</T><A>https://www.youtube.com/watch?v=ugvHCXCOmm4</A><L>en</L><F>MP4</F><DURATION><HOUR>5</HOUR><MINUTE>15</MINUTE><SECOND>0</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Dario</FIRSTNAME><LASTNAME>Amodei</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lex</FIRSTNAME><LASTNAME>Fridman</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>11</MONTH><DAY>11</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Dario</FIRSTNAME><LASTNAME>Amodei</LASTNAME></AUTHOR> describes his vision of LLMs, <AUTHOR><FIRSTNAME>Amanda</FIRSTNAME><LASTNAME>Askell</LASTNAME></AUTHOR> explains how she helps defining Claude temperament, and <AUTHOR><FIRSTNAME>Chris</FIRSTNAME><LASTNAME>Olah</LASTNAME></AUTHOR> explains Mechanistic Interpretability.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>🔥 This CHANGES the REASONING Game!!!💥 Nous Forge Reasoning💥</T><A>https://www.youtube.com/watch?v=SOR_toP9_Ag</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>6</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>11</MONTH><DAY>12</DAY></DATE><COMMENT>A usual <AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR> reading of an announcement: Nous’ Forge Reasoning API, yet another try to get better results by using Monte Carlo Tree Search, Chain of Code, and Mixture of Agents.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Small Language Models, Synthetic Data and Robotics at the opening of Web Summit 2024</T><A>https://www.youtube.com/watch?v=FoWpbpgyO18</A><L>en</L><F>MP4</F><DURATION><MINUTE>18</MINUTE><SECOND>27</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Thomas</FIRSTNAME><LASTNAME>Wolf</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>11</MONTH><DAY>15</DAY></DATE><COMMENT>Some thoughts about the interest of small language models.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>New Pleias 1.0 LLMs trained exclusively on openly licensed data</T><A>https://simonwillison.net/2024/Dec/5/pleias-llms/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>12</MONTH><DAY>5</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>A Deep Dive Into The RedPajama Datasets</T><A>https://www.youtube.com/watch?v=_HFxuQUg51k</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>1</MINUTE><SECOND>28</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Maurice</FIRSTNAME><LASTNAME>Weber</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Zain</FIRSTNAME><LASTNAME>Hasan</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>12</MONTH><DAY>6</DAY></DATE><COMMENT>Some information on how the RedPajama Dataseset has been built.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Things we learned about LLMs in 2024</T><A>https://simonwillison.net/2024/Dec/31/llms-in-2024/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>12</MONTH><DAY>31</DAY></DATE><COMMENT>A summary of the year.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>How to OPTIMIZE your prompts for better Reasoning!</T><A>https://www.youtube.com/watch?v=IVRr3zvXC0Q</A><L>en</L><F>MP4</F><DURATION><MINUTE>21</MINUTE><SECOND>16</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Sam</FIRSTNAME><LASTNAME>Witteveen</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>1</MONTH><DAY>9</DAY></DATE><COMMENT>A presentation of PromptWizard, a Microsoft open-source framework to optimise prompts.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>LLM Lecture: A Deep Dive into Transformers, Prompts, and Human Feedback</T><A>https://www.youtube.com/watch?v=BprirYymXrg</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>31</MINUTE><SECOND>8</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>1</MONTH><DAY>19</DAY></DATE><COMMENT>A wide and good overview of how LLMs are implemented. But this is a lot of information in too little time. If you do not know about the matter, I guess you will have trouble to understand everything.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>What is a Context Window? Unlocking LLM Secrets</T><A>https://www.youtube.com/watch?v=-QVoIxEpFkM</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>30</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Martin</FIRSTNAME><LASTNAME>Keen</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>1</MONTH><DAY>21</DAY></DATE><COMMENT>A very basic explanation of context.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>Deep Dive into LLMs like ChatGPT</T><A>https://www.youtube.com/watch?v=7xTGNNLPyMI</A><L>en</L><F>MP4</F><DURATION><HOUR>3</HOUR><MINUTE>31</MINUTE><SECOND>23</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>2</MONTH><DAY>5</DAY></DATE><COMMENT>A long, clear, and non technical description of how chat AI are built.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>How I use LLMs</T><A>https://www.youtube.com/watch?v=EWvNQjAaOHw</A><L>en</L><F>MP4</F><DURATION><HOUR>2</HOUR><MINUTE>11</MINUTE><SECOND>11</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>2</MONTH><DAY>27</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR> describes his use of GenAI, he uses mostly OpenAI.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Will AI Ever Understand Language Like Humans?</T><ST>AI may sound like a human, but that doesn’t mean that AI learns like a human. In this episode, Ellie Pavlick explains why understanding how LLMs can process language could unlock deeper insights into both AI and the human mind.</ST><A>https://www.quantamagazine.org/will-ai-ever-understand-language-like-humans-20250501/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Ellie</FIRSTNAME><LASTNAME>Pavlick</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Steven</FIRSTNAME><LASTNAME>Strogatz</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Janna</FIRSTNAME><LASTNAME>Levin</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>1</DAY></DATE><COMMENT>There is nothing new in this interview, just generalities about LLMs.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Coding LLMs from the Ground Up: A Complete Course</T><A>https://magazine.sebastianraschka.com/p/coding-llms-from-the-ground-up</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>10</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Sebastian</FIRSTNAME><LASTNAME>Raschka</LASTNAME></AUTHOR> lists the videos of his "Build a Large Language Model (From Scratch)" series.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Trying out llama.cpp’s new vision support</T><A>https://simonwillison.net/2025/May/10/llama-cpp-vision/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>10</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR> is experimenting with llama.cpp and <X><T>unsloth/gemma-3-4b-it-GGUF</T><A>https://huggingface.co/unsloth/gemma-3-4b-it-GGUF</A><L>en</L><F>HTML</F></X>.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>How often do LLMs snitch? Recreating Theo’s SnitchBench with LLM</T><A>https://simonwillison.net/2025/May/31/snitchbench-with-llm/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>31</DAY></DATE><COMMENT>Models can act as whistler blowers when being asked to apply their values and having access to communication tooks.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Faster LLMs: Accelerate Inference with Speculative Decoding</T><A>https://www.youtube.com/watch?v=VkWlLSTdHs8</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>38</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Isaac</FIRSTNAME><LASTNAME>Ke</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>4</DAY></DATE><COMMENT>A description of speculative decoding.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Andrej Karpathy: Software Is Changing (Again)</T><A>https://www.youtube.com/watch?v=LCEmiRjPEtQ</A><L>en</L><F>MP4</F><DURATION><MINUTE>39</MINUTE><SECOND>31</SECOND></DURATION><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>19</DAY></DATE></X><AUTHOR><FIRSTNAME>Andrej</FIRSTNAME><LASTNAME>Karpathy</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>17</DAY></DATE><COMMENT>Some ideas about LLMs: a new software paradigm, an analogy with operating systems, the autonomy slider, accelerating the LLM/human loop, and making data more easily usable by LLMs.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Context engineering</T><A>https://simonwillison.net/2025/Jun/27/context-engineering/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>27</DAY></DATE><COMMENT>Some persons thinks that "Context Engineering" is a better term than "Prompt Engineering".</COMMENT></ARTICLE></ITEM>
    <ITEM><BLIST><TITLE>Chatbot Arena</TITLE>
      <ITEM><ARTICLE><X><T>A much better LLM Leaderboard!!!</T><A>https://www.youtube.com/watch?v=_RBAsItC6CU</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>23</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>28</DAY></DATE><COMMENT>A presentation of Chatbot Arena.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Chatbot Arena: New models &amp; Elo system update</T><A>https://lmsys.org/blog/2023-12-07-leaderboard/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Wei-Lin</FIRSTNAME><LASTNAME>Chiang</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Tim</FIRSTNAME><LASTNAME>Li</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Joseph</FIRSTNAME><MIDDLENAME>E.</MIDDLENAME><LASTNAME>Gonzalez</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Ion</FIRSTNAME><LASTNAME>Stoica</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>7</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>LMSYS Chatbot Arena: Live and Community-Driven LLM Evaluation</T><A>https://lmsys.org/blog/2024-03-01-policy/</A><L>en</L><F>HTML</F></X><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>1</DAY></DATE><COMMENT>A presentation of Chatbot Arena by its authors.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>From Live Data to High-Quality Benchmarks: The Arena-Hard Pipeline</T><A>https://lmsys.org/blog/2024-04-19-arena-hard/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Tianle</FIRSTNAME><LASTNAME>Li</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Wei-Lin</FIRSTNAME><LASTNAME>Chiang</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Evan</FIRSTNAME><LASTNAME>Frick</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lisa</FIRSTNAME><LASTNAME>Dunlap</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Banghua</FIRSTNAME><LASTNAME>Zhu</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Joseph</FIRSTNAME><MIDDLENAME>E.</MIDDLENAME><LASTNAME>Gonzalez</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Ion</FIRSTNAME><LASTNAME>Stoica</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>19</DAY></DATE><COMMENT>A detailed description of Arena-Hard, a rather complex comparison mechanism trying to correctly evaluate and force differentiation in scoring chatbot.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Introducing Hard Prompts Category in Chatbot Arena</T><A>https://lmsys.org/blog/2024-05-17-category-hard/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Tianle</FIRSTNAME><LASTNAME>Li</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Wei-Lin</FIRSTNAME><LASTNAME>Chiang</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>17</DAY></DATE><COMMENT>Some first results of Arena-Hard.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>The Multimodal Arena is Here!</T><A>https://lmsys.org/blog/2024-06-27-multimodal/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Christopher</FIRSTNAME><LASTNAME>Chou</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lisa</FIRSTNAME><LASTNAME>Dunlap</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Wei-Lin</FIRSTNAME><LASTNAME>Chiang</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Ying</FIRSTNAME><LASTNAME>Sheng</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lianmin</FIRSTNAME><LASTNAME>Zheng</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Anastasios</FIRSTNAME><LASTNAME>Angelopoulos</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Trevor</FIRSTNAME><LASTNAME>Darrell</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Ion</FIRSTNAME><LASTNAME>Stoica</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Joseph</FIRSTNAME><MIDDLENAME>E.</MIDDLENAME><LASTNAME>Gonzalez</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>27</DAY></DATE><COMMENT>Chatbot Arena now support images.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>RedTeam Arena: An Open-Source, Community-driven Jailbreaking Platform</T><A>https://lmsys.org/blog/2024-09-13-redteam-arena/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Anastasios</FIRSTNAME><LASTNAME>Angelopoulos</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lucas</FIRSTNAME><LASTNAME>Vivona</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Wei-Lin</FIRSTNAME><LASTNAME>Chiang</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Aryan</FIRSTNAME><LASTNAME>Vichare</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lisa</FIRSTNAME><LASTNAME>Dunlap</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Salvivona</FIRSTNAME></AUTHOR><AUTHOR><GIVENNAME>Pliny</GIVENNAME></AUTHOR><AUTHOR><FIRSTNAME>Ion</FIRSTNAME><LASTNAME>Stoica</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>9</MONTH><DAY>13</DAY></DATE><COMMENT>RedTeam Arena tries to evaluate how difficult it is to jailbreaking models. It generates two leaderboards: one for the model, the other one for the gamers. But the first game is so basic that it has little value.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>WebDev: This FREE AI Coder BEATS V0, Bolt &amp; Has 3.5 SONNET, GPT-4O &amp; More FOR FREE!</T><A>https://www.youtube.com/watch?v=aZiEFlvN7n0</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>35</SECOND></DURATION></X><AUTHOR><GIVENNAME>AICodeKing</GIVENNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>12</MONTH><DAY>14</DAY></DATE><COMMENT>A presentation of WebDev Arena, an arena to benchmark models for web development tasks.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>WebDev Arena</T><A>https://simonwillison.net/2024/Dec/16/webdev-arena/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>12</MONTH><DAY>16</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR> has extracted the system prompt of WebDev Arena.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Understanding the recent criticism of the Chatbot Arena</T><A>https://simonwillison.net/2025/Apr/30/criticism-of-the-chatbot-arena/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>4</MONTH><DAY>30</DAY></DATE><COMMENT>Large companies are gaming Chatbot Arena.</COMMENT></ARTICLE></ITEM>
    </BLIST></ITEM>
    <ITEM><BLIST><TITLE>Model parameter extraction</TITLE>
      <ITEM><ARTICLE><X quality="-2"><T>Stealing bit of GPT's Brain for $20?!!! (INSANE GOOGLE RESEARCH)</T><A>https://www.youtube.com/watch?v=JUP1O1qlVi4</A><L>en</L><F>MP4</F><DURATION><MINUTE>23</MINUTE><SECOND>6</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>12</DAY></DATE><COMMENT>A slow and very bad explanation of "<X><T>Stealing Part of a Production Language Model</T><A>https://arxiv.org/abs/2403.06634</A><L>en</L><F>PDF</F></X>" paper.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Stealing Part of a Production LLM | API protects LLMs no more</T><A>https://www.youtube.com/watch?v=O_eUzrFU6eQ</A><L>en</L><F>MP4</F><DURATION><MINUTE>18</MINUTE><SECOND>49</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Letitia</FIRSTNAME><LASTNAME>Parcalabescu</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>8</DAY></DATE><COMMENT>A better explanation of the method used in the previous paper and in "<X><T>Logits of API-Protected LLMs Leak Proprietary Information</T><A>https://arxiv.org/abs/2403.09539</A><L>en</L><F>PDF</F></X>".</COMMENT></ARTICLE></ITEM>
    </BLIST></ITEM>
    <ITEM><BLIST><TITLE>SGLang</TITLE>
      <ITEM><ARTICLE><X><T>Achieving Faster Open-Source Llama3 Serving with SGLang Runtime (vs. TensorRT-LLM, vLLM)</T><A>https://lmsys.org/blog/2024-07-25-sglang-llama3/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Liangsheng</FIRSTNAME><LASTNAME>Yin</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Yineng</FIRSTNAME><LASTNAME>Zhang</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Ying</FIRSTNAME><LASTNAME>Sheng</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>7</MONTH><DAY>25</DAY></DATE><COMMENT>LMSYS has created a new server to host chat and vision servers, and they are proud of its performance.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>SGLang v0.3 Release: 7x Faster DeepSeek MLA, 1.5x Faster torch.compile, Multi-Image/Video LLaVA-OneVision</T><A>https://lmsys.org/blog/2024-09-04-sglang-v0-3/</A><L>en</L><F>HTML</F></X><DATE><YEAR>2024</YEAR><MONTH>9</MONTH><DAY>4</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>SGLang v0.4: Zero-Overhead Batch Scheduler, Cache-Aware Load Balancer, Faster Structured Outputs</T><A>https://lmsys.org/blog/2024-12-04-sglang-v0-4/</A><L>en</L><F>HTML</F></X><DATE><YEAR>2024</YEAR><MONTH>12</MONTH><DAY>4</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Deploying DeepSeek with PD Disaggregation and Large-Scale Expert Parallelism on 96 H100 GPUs</T><A>https://lmsys.org/blog/2025-05-05-large-scale-ep/</A><L>en</L><F>HTML</F></X><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>5</DAY></DATE><COMMENT>A detailed and very technical description of the SGLang support of DeepSeek and achieved performance.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Transformers backend integration in SGLang</T><A>https://huggingface.co/blog/transformers-backend-sglang</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Yineng</FIRSTNAME><LASTNAME>Zhang</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Ke</FIRSTNAME><LASTNAME>Bao</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lianmin</FIRSTNAME><LASTNAME>Zheng</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Jin</FIRSTNAME><LASTNAME>Pan</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Marc</FIRSTNAME><LASTNAME>Sun</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>23</DAY></DATE><COMMENT>SGLang now supports (Hugging Face’s) transformers as a backend.</COMMENT></ARTICLE></ITEM>
    </BLIST></ITEM>
    <ITEM><BLIST><TITLE>OpenAssistant</TITLE>
      <ITEM><ARTICLE><X quality="-1"><T>OpenAssistant - ChatGPT's Open Alternative (We need your help!)</T><A>https://www.youtube.com/watch?v=64Izfm24FKA</A><L>en</L><F>MP4</F><DURATION><MINUTE>35</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>2</MONTH><DAY>4</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR> is trying to motivate persons to contribute training data to his project, a kind of volunteer Mechanical Turk.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>OpenAssistant First Models are here! (Open-Source ChatGPT)</T><A>https://www.youtube.com/watch?v=Hi6cbeBY2oQ</A><L>en</L><F>MP4</F><DURATION><MINUTE>16</MINUTE><SECOND>52</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>7</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>OpenAssistant RELEASED! The world's best open-source Chat AI!</T><A>https://www.youtube.com/watch?v=ddG2fM9i4Kk</A><L>en</L><F>MP4</F><DURATION><MINUTE>21</MINUTE><SECOND>5</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>4</MONTH><DAY>15</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>OpenAssistant is Completed</T><A>https://www.youtube.com/watch?v=gqtmUHhaplo</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>48</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Yannic</FIRSTNAME><LASTNAME>Kilcher</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>10</MONTH><DAY>24</DAY></DATE><COMMENT>The project is ending.</COMMENT></ARTICLE></ITEM>
    </BLIST></ITEM>
  </BLIST></ITEM>
</LLIST>
</CONTENT>
</PAGE>