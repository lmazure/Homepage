<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="../css/strict.xsl"?>
<PAGE xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:noNamespaceSchemaLocation="../css/schema.xsd" xml:lang="en">
<TITLE>AI security/safety</TITLE>
<PATH>links/aisecurity.xml</PATH>
<DATE><YEAR>2025</YEAR><MONTH>10</MONTH><DAY>3</DAY></DATE>
<CONTENT>
<LLIST>
  <ITEM><SLIST>
    <ITEM><X><T>MITRE ATLAS</T><A>https://atlas.mitre.org/</A><L>en</L><F>HTML</F></X></ITEM>
    <ITEM><X><T>NIST - Adversarial Machine Learning - A Taxonomy and Terminology of Attacks and Mitigations</T><A>https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-2e2025.pdf</A><L>en</L><F>PDF</F></X></ITEM>
    <ITEM><X><T>OWASP Top Ten</T><A>https://genai.owasp.org/llm-top-10/</A><L>en</L><F>HTML</F></X></ITEM>
    <ITEM><X><T>OWASP AI Exchange</T><A>https://owaspai.org/</A><L>en</L><F>HTML</F></X></ITEM>
  </SLIST></ITEM>
  <ITEM><BLIST><TITLE>Articles and videos</TITLE>
    <ITEM><ARTICLE><X><T>Deadly Truth of General AI? - Computerphile</T><A>https://www.youtube.com/watch?v=tcdVC4e6EV4</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>30</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2015</YEAR><MONTH>6</MONTH><DAY>17</DAY></DATE><COMMENT>A science-fiction story to explain that an AI may do unexpected things in order to optimise its objective.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI Self Improvement - Computerphile</T><A>https://www.youtube.com/watch?v=5qfIgCiYlfY</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>20</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2015</YEAR><MONTH>7</MONTH><DAY>24</DAY></DATE><COMMENT>The possibility of a critical incident with a self-improving AI.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI can be sexist and racist — it’s time to make it fair</T><ST>Computer scientists must identify sources of bias, de-bias training data and develop artificial-intelligence algorithms that are robust to skews in the data, argue James Zou and Londa Schiebinger.</ST><A>https://www.nature.com/articles/d41586-018-05707-8</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>James</FIRSTNAME><LASTNAME>Zou</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Londa</FIRSTNAME><LASTNAME>Schiebinger</LASTNAME></AUTHOR><DATE><YEAR>2018</YEAR><MONTH>7</MONTH><DAY>18</DAY></DATE><COMMENT>AI is biased because the data on which it is trained is biased. The authors suggest some directions to mitigate this.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Three Small Stickers in Intersection Can Cause Tesla Autopilot to Swerve Into Wrong Lane</T><ST>Security researchers from Tencent have demonstrated a way to use physical attacks to spoof Tesla's autopilot</ST><A>https://spectrum.ieee.org/three-small-stickers-on-road-can-steer-tesla-autopilot-into-oncoming-lane</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Evan</FIRSTNAME><LASTNAME>Ackerman</LASTNAME></AUTHOR><DATE><YEAR>2019</YEAR><MONTH>4</MONTH><DAY>1</DAY></DATE><COMMENT>Some Japanese researchers are able to trick Tesla’s lane detection algorithm with some coloured patches.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI Safety Gym - Computerphile</T><A>https://www.youtube.com/watch?v=31rU-VzF5ww</A><L>en</L><F>MP4</F><DURATION><MINUTE>15</MINUTE><SECOND>59</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2020</YEAR><MONTH>1</MONTH><DAY>30</DAY></DATE><COMMENT>The problem of having safe AIs and the Safety Gym benchmark proposed by OpenAI.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>L'éthique des algorithmes en sérieux danger</T><A>https://www.youtube.com/watch?v=Ddr-BZ9W180</A><L>fr</L><F>MP4</F><DURATION><MINUTE>7</MINUTE><SECOND>21</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR><DATE><YEAR>2020</YEAR><MONTH>12</MONTH><DAY>7</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Timnit</FIRSTNAME><LASTNAME>Gebru</LASTNAME></AUTHOR> has been fired by Google, <AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR> states once again the importance of research on the ethics of AI.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://www.youtube.com/watch?v=Ddr-BZ9W180"><X><T>Google démantèle son éthique (et tout le monde s'en fout...)</T><A>https://www.youtube.com/watch?v=HbFadtOxs4k</A><L>fr</L><F>MP4</F><DURATION><MINUTE>7</MINUTE><SECOND>57</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR><DATE><YEAR>2021</YEAR><MONTH>2</MONTH><DAY>22</DAY></DATE><COMMENT>Two months later, Google also fires <AUTHOR><FIRSTNAME>Margarett</FIRSTNAME><LASTNAME>Mitchell</LASTNAME></AUTHOR>.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://www.youtube.com/watch?v=HbFadtOxs4k"><X><T>What Really Happened When Google Ousted Timnit Gebru</T><ST>She was a star engineer who warned that messy AI can spread racism. Google brought her in. Then it forced her out. Can Big Tech take criticism from within?</ST><A>https://www.wired.com/story/google-timnit-gebru-ai-what-really-happened/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Tom</FIRSTNAME><LASTNAME>Simonite</LASTNAME></AUTHOR><DATE><YEAR>2021</YEAR><MONTH>6</MONTH><DAY>8</DAY></DATE><COMMENT>A long article about <AUTHOR><FIRSTNAME>Timnit</FIRSTNAME><LASTNAME>Gebru</LASTNAME></AUTHOR>: her biography, what she did at Google, and her firing.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://www.wired.com/story/google-timnit-gebru-ai-what-really-happened/"><X><T>On Racialized Tech Organizations and Complaint: A Goodbye to Google</T><A>https://alex-hanna.medium.com/on-racialized-tech-organizations-and-complaint-a-goodbye-to-google-43fd8045991d</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Alex</FIRSTNAME><LASTNAME>Hanna</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>2</MONTH><DAY>2</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Alex</FIRSTNAME><LASTNAME>Hanna</LASTNAME></AUTHOR> explains why she resigns from Google because of the whiteness of its organisational practices.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>We Were Right! Real Inner Misalignment</T><A>https://www.youtube.com/watch?v=zkbPdEHEyEI</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>46</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2021</YEAR><MONTH>10</MONTH><DAY>10</DAY></DATE><COMMENT>The problem when an AI system is not targeting the expected objective.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI’s 6 Worst-Case Scenarios</T><ST>Who needs Terminators when you have precision clickbait and ultra-deepfakes?</ST><A>https://spectrum.ieee.org/ai-worst-case-scenarios</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Natasha</FIRSTNAME><LASTNAME>Bajema</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>1</MONTH><DAY>3</DAY></DATE><COMMENT>A list of problems with AI. There is nothing new here: these issues are often talked about.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>TAY : L'INTELLIGENCE ARTIFICIELLE DEVENUE NAZIE EN 24H SUR TWITTER</T><A>https://www.youtube.com/watch?v=mtpanIOCRQw</A><L>fr</L><F>MP4</F><DURATION><MINUTE>28</MINUTE><SECOND>15</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Pacôme</FIRSTNAME><LASTNAME>Thiellement</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>6</MONTH><DAY>17</DAY></DATE><COMMENT>These facts about trolling AIs and real humans are well-known and this description is much too long and too slow.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Tricking AI Image Recognition - Computerphile</T><A>https://www.youtube.com/watch?v=gGIiechWEFs</A><L>en</L><F>MP4</F><DURATION><MINUTE>12</MINUTE><SECOND>31</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Alex</FIRSTNAME><LASTNAME>Turner</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>7</MONTH><DAY>27</DAY></DATE><COMMENT>Changing a few pixels in an image to deceive a neural network.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Civilian AI Is Already Being Misused by the Bad Guys</T><ST>And the AI community needs to do something about it</ST><A>https://spectrum.ieee.org/responsible-ai-threat</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Vincent</FIRSTNAME><LASTNAME>Boulanin</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Charles</FIRSTNAME><LASTNAME>Ovink</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>8</MONTH><DAY>27</DAY></DATE><COMMENT>The authors ask AI researchers to consider the risks that their work may be misused for disinformation, war, or terrorism, and the options for limiting these risks.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Biden proposes new “Bill of Rights” to protect Americans from AI harms</T><ST>Non-binding national guidelines on AI harms may inform future policy and business decisions.</ST><A>https://arstechnica.com/information-technology/2022/10/biden-proposes-new-bill-of-rights-to-protect-americans-from-ai-snooping/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>10</MONTH><DAY>4</DAY></DATE><COMMENT>The subtitle says it all.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Google a empoisonné la communauté scientifique, qui amplifie maintenant sa désinformation</T><A>https://www.youtube.com/watch?v=IVqXKP91L4E</A><L>fr</L><F>MP4</F><DURATION><MINUTE>57</MINUTE><SECOND>8</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>10</MONTH><DAY>17</DAY></DATE><COMMENT>The problem with Google’s articles on AI, but I am not sure these are evil acts, maybe this is just the shortsightedness of researchers.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Why Does AI Lie, and What Can We Do About It?</T><A>https://www.youtube.com/watch?v=w65p_IIp6JY</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>23</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2022</YEAR><MONTH>12</MONTH><DAY>9</DAY></DATE><COMMENT>Training on a larger data set does not mean the model will be more truthful.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>How ChatGPT Hijacks Democracy</T><A>https://www.nytimes.com/2023/01/15/opinion/ai-chatgpt-lobbying-democracy.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Nathan</FIRSTNAME><MIDDLENAME>E.</MIDDLENAME><LASTNAME>Sanders</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>1</MONTH><DAY>15</DAY></DATE><COMMENT>How AI could be used by lobbyists, not only to generate textual content, but also to find the best persons to target.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://www.nytimes.com/2023/01/15/opinion/ai-chatgpt-lobbying-democracy.html"><X><T>ChatGPT Wrote (Most of) This Letter</T><A>https://www.nytimes.com/2023/01/24/opinion/letters/democracy-chatbot.html</A><L>en</L><F>HTML</F></X><DATE><YEAR>2023</YEAR><MONTH>1</MONTH><DAY>24</DAY></DATE><COMMENT>An answer generated by ChatGPT.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://www.nytimes.com/2023/01/24/opinion/letters/democracy-chatbot.html"><X><T>We Don’t Need to Reinvent our Democracy to Save it from AI</T><A>https://www.belfercenter.org/publication/we-dont-need-reinvent-our-democracy-save-it-ai</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Nathan</FIRSTNAME><LASTNAME>Sanders</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>2</MONTH><DAY>9</DAY></DATE><COMMENT>Technical solutions or regulation will not protect the democracy from the AI-assisted lobbying. Limiting contribution to political parties, transparency, and motivating people to participate in democracy would help.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Le vrai danger avec #ChatGPT</T><A>https://www.youtube.com/watch?v=QprkRP-Dylo</A><L>fr</L><F>MP4</F><DURATION><MINUTE>21</MINUTE><SECOND>29</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>1</MONTH><DAY>23</DAY></DATE><COMMENT>A usual video from <AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR>: AI such as ChatGPT brings more risks, in particular given that it will be used for cybercrime, than benefits.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI applications open new security vulnerabilities</T><ST>Your ML model and AI-as-a-service apps might open new attack surfaces. Here's how to mitigate them.</ST><A>https://stackoverflow.blog/2023/01/24/ai-applications-open-new-security-vulnerabilities/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Taimur</FIRSTNAME><LASTNAME>Ijlal</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>1</MONTH><DAY>24</DAY></DATE><COMMENT>An introduction to the possible attacks of AI-based applications.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Attacking Machine Learning Systems</T><A>https://www.schneier.com/blog/archives/2023/02/attacking-machine-learning-systems.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>2</MONTH><DAY>6</DAY></DATE><COMMENT>Some musing about attacks against ML systems and the fact that, at the end, the weakness will be in the software surrounding the AI.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Hacking the Tax Code</T><A>https://www.schneier.com/blog/archives/2023/02/hacking-the-tax-code.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>2</MONTH><DAY>10</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR> imagines that AI could be used to find loopholes in tax laws.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Don’t worry about AI breaking out of its box—worry about us breaking in</T><ST>Opinion: The worst human impulses will find plenty of uses for generative AI.</ST><A>https://arstechnica.com/gadgets/2023/02/dont-worry-about-ai-breaking-out-of-its-box-worry-about-us-breaking-in/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Rob</FIRSTNAME><LASTNAME>Reid</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>2</MONTH><DAY>24</DAY></DATE><COMMENT>The risks with AIs are not the AIs themselves, but the humans using them for bad purposes.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Report: Microsoft cut a key AI ethics team</T><ST>Expert calls decision "damning," says it's time for regulators to get involved.</ST><A>https://arstechnica.com/tech-policy/2023/03/amid-bing-chat-controversy-microsoft-cut-an-ai-ethics-team-report-says/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Ashley</FIRSTNAME><LASTNAME>Belanger</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>14</DAY></DATE><COMMENT>Microsoft eliminated one of its AI ethics teams: the ethics and society team.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>OpenAI checked to see whether GPT-4 could take over the world</T><ST>"ARC's evaluation has much lower probability of leading to an AI takeover than the deployment itself."</ST><A>https://arstechnica.com/information-technology/2023/03/openai-checked-to-see-whether-gpt-4-could-take-over-the-world/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>15</DAY></DATE><COMMENT>The Alignment Research Center analysed the existential risk (X-risk) from a misaligned GPT-4.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Protecting AI Models from “Data Poisoning”</T><ST>New ways to thwart backdoor control of deep learning systems</ST><A>https://spectrum.ieee.org/ai-cybersecurity-data-poisoning</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Payal</FIRSTNAME><LASTNAME>Dhar</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>24</DAY></DATE><COMMENT>Two examples of how AI training data could be manipulated.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Fearing “loss of control,” AI critics call for 6-month pause in AI development</T><ST>"This pause should be public and verifiable, and include all key actors."</ST><A>https://arstechnica.com/information-technology/2023/03/fearing-loss-of-control-ai-critics-call-for-6-month-pause-in-ai-development/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>29</DAY></DATE><COMMENT>Different reactions to the current fast-paced AI progress, from AI hype to basically explaining that LLMs are biased and generate wrong information.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Warning of AI’s danger, pioneer Geoffrey Hinton quits Google to speak freely</T><ST>"Most people thought it was way off. And I thought it was way off."</ST><A>https://arstechnica.com/information-technology/2023/05/warning-of-ais-danger-pioneer-geoffrey-hinton-quits-google-to-speak-freely/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>5</MONTH><DAY>1</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Geoffrey</FIRSTNAME><LASTNAME>Hinton</LASTNAME></AUTHOR> resigned from Google so he can speak freely about AI risks.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Can We Build Trustworthy AI?</T><ST>AI isn't transparent, so we should all be preparing for a world where AI is not trustworthy, write two Harvard researchers.</ST><A>https://gizmodo.com/ai-chatgpt-can-we-build-trustworthy-ai-1850405280</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Nathan</FIRSTNAME><MIDDLENAME>E.</MIDDLENAME><LASTNAME>Sanders</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>5</MONTH><DAY>4</DAY></DATE><COMMENT>The usual <AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR>’s message: AI is useful, but it must prove that it can be trusted.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>OpenAI execs warn of “risk of extinction” from artificial intelligence in new open letter</T><ST>Strategically vague statement on AI risk prompts critics' response.</ST><A>https://arstechnica.com/information-technology/2023/05/openai-execs-warn-of-risk-of-extinction-from-artificial-intelligence-in-new-open-letter/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>5</MONTH><DAY>30</DAY></DATE><COMMENT>A new round of bullshit.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://arstechnica.com/information-technology/2023/05/openai-execs-warn-of-risk-of-extinction-from-artificial-intelligence-in-new-open-letter/"><X><T>On the Catastrophic Risk of AI</T><A>https://www.schneier.com/blog/archives/2023/06/on-the-catastrophic-risk-of-ai.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>6</MONTH><DAY>1</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR> regrets to have signed the statement.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Spotify a payé leur silence</T><A>https://www.youtube.com/watch?v=eglvOnpwKmo</A><L>fr</L><F>MP4</F><DURATION><MINUTE>21</MINUTE><SECOND>21</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>6</MONTH><DAY>12</DAY></DATE><COMMENT>Yet another video about the problem of hacking recommendation or gain sharing algorithms.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>The AI Apocalypse: A Scorecard</T><ST>How worried are top AI experts about the threat posed by large language models like GPT-4?</ST><A>https://spectrum.ieee.org/artificial-general-intelligence</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Eliza</FIRSTNAME><LASTNAME>Strickland</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Glenn</FIRSTNAME><LASTNAME>Zorpette</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>6</MONTH><DAY>21</DAY></DATE><COMMENT>A short overview of the opinions of some AI leaders on the fact that LLMs are a step toward AGI and the existential risks of AI.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>The A.I. Wars Have Three Factions, and They All Crave Power</T><A>https://www.nytimes.com/2023/09/28/opinion/ai-safety-ethics-effective.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Nathan</FIRSTNAME><MIDDLENAME>E.</MIDDLENAME><LASTNAME>Sanders</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>9</MONTH><DAY>28</DAY></DATE><COMMENT>The doomsayers (who speak about an existential risk), the reformers (who denounce the current AI sexist, racist… biases), and the warrior (who push for an AI arm race with China).</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>“Catastrophic” AI harms among warnings in declaration signed by 28 nations</T><ST>"Bletchley Declaration" sums up first day of UK's international AI Safety Summit.</ST><A>https://arstechnica.com/information-technology/2023/11/catastrophic-ai-harms-among-warnings-in-declaration-signed-by-28-nations/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Benj</FIRSTNAME><LASTNAME>Edwards</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>1</DAY></DATE><COMMENT>I am not sure this summit will have any useful effect.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>How Deepfakes Fuel Conspiracy Theories</T><ST>When official accounts post AI-generated videos, they give reason to doubt in general</ST><A>https://spectrum.ieee.org/deepfake-2666142928</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Rahul</FIRSTNAME><LASTNAME>Rao</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>9</DAY></DATE><COMMENT>The problem with deepfakes: some people believe that some true videos are fake.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI Art Generators Can Be Fooled Into Making NSFW Images</T><ST>Nonsense words can get around DALLE-2’s and Stable Diffusion’s filters</ST><A>https://spectrum.ieee.org/dall-e</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Charles</FIRSTNAME><MIDDLENAME>Q.</MIDDLENAME><LASTNAME>Choi</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>20</DAY></DATE><COMMENT>Still more prompt hacking, this one uses words which are misunderstood.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI and Trust</T><A>https://www.belfercenter.org/publication/ai-and-trust</A><L>en</L><F>HTML</F></X><X><T>AI and Trust</T><A>https://www.schneier.com/blog/archives/2023/12/ai-and-trust.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>27</DAY></DATE><COMMENT>After describing the different types of trust, mostly interpersonal trust and social trust, <AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR> explains that social trust must be applicable to AI and governments need to constrain the behaviour of AI corporations.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>ok! this is scary!!! (LLM Sleeper Agents)</T><A>https://www.youtube.com/watch?v=CdFUh0pmqNI</A><L>en</L><F>MP4</F><DURATION><MINUTE>21</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Abdul Majed</FIRSTNAME><LASTNAME>Raja</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>1</MONTH><DAY>13</DAY></DATE><COMMENT>A paper ("<X><T>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</T><A>https://arxiv.org/abs/2401.05566</A><L>en</L><F>HTML</F></X>") describes how a poisoned model, trained to generate malicious output when triggered with some given input, can go undetected and cannot be made safe by training it.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>ASCII art elicits harmful responses from 5 major AI chatbots</T><ST>LLMs are trained to block harmful responses. Old-school images can override those rules.</ST><A>https://arstechnica.com/security/2024/03/researchers-use-ascii-art-to-elicit-harmful-responses-from-5-major-ai-chatbots/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Dan</FIRSTNAME><LASTNAME>Goodin</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>16</DAY></DATE><COMMENT>Yet another mechanism to jailbreak LLMs.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Adversarial Machine Learning</T><A>https://oxide-and-friends.transistor.fm/episodes/adversarial-machine-learning</A><L>en</L><F>MP3</F><DURATION><HOUR>1</HOUR><MINUTE>23</MINUTE><SECOND>30</SECOND></DURATION><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>27</DAY></DATE></X><X><T>Oxide and Friends 3/25/2024 -- Adversarial Machine Learning</T><A>https://www.youtube.com/watch?v=ATth8p-7Jnc</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>23</MINUTE><SECOND>29</SECOND></DURATION><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>26</DAY></DATE></X><AUTHOR><FIRSTNAME>Nicholas</FIRSTNAME><LASTNAME>Carlini</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Bryan</FIRSTNAME><LASTNAME>Cantrill</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Adam</FIRSTNAME><LASTNAME>Leventhal</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>25</DAY></DATE><COMMENT>An informal discussion about hacking AI models.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>ChatGPT Jailbreak - Computerphile</T><A>https://www.youtube.com/watch?v=zn2ukSnDqSg</A><L>en</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>40</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Tim</FIRSTNAME><LASTNAME>Muller</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>9</DAY></DATE><COMMENT>A late and not so good description of jail breaking and prompt injection.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Faking William Morris, Generative Forgery, and the Erosion of Art History</T><ST>Buying fake William Morris prints on Etsy and other early signs of epistemological collapse</ST><A>https://maggieappleton.com/generative-forgery</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Maggie</FIRSTNAME><LASTNAME>Appleton</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>4</MONTH><DAY>30</DAY></DATE><COMMENT>The subtitle says it all.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Russia and China are using OpenAI tools to spread disinformation</T><ST>Iran and Israel have been getting in on the action as well.</ST><A>https://arstechnica.com/ai/2024/05/russia-and-china-are-using-openai-tools-to-spread-disinformation/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Hannah</FIRSTNAME><LASTNAME>Murphy</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>5</MONTH><DAY>31</DAY></DATE><COMMENT>The most interesting part of the article is the fact that OpenAI says it tracks foreign usage and it speaks about it.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI Ruined My Year</T><A>https://www.youtube.com/watch?v=2ziuPUeewK0</A><L>en</L><F>MP4</F><DURATION><MINUTE>45</MINUTE><SECOND>58</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>1</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR> appreciates that AI safety is now spoken about.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>#431 – Roman Yampolskiy: Dangers of Superintelligent AI</T><A>https://lexfridman.com/roman-yampolskiy</A><L>en</L><F>MP3</F><DURATION><HOUR>2</HOUR><MINUTE>22</MINUTE><SECOND>28</SECOND></DURATION></X><X quality="-1"><T>Roman Yampolskiy: Dangers of Superintelligent AI | Lex Fridman Podcast #431</T><A>https://www.youtube.com/watch?v=NNr6gPelJ3E</A><L>en</L><F>MP4</F><DURATION><HOUR>2</HOUR><MINUTE>15</MINUTE><SECOND>38</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Roman</FIRSTNAME><LASTNAME>Yampolskiy</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Lex</FIRSTNAME><LASTNAME>Fridman</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>2</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Roman</FIRSTNAME><LASTNAME>Yampolskiy</LASTNAME></AUTHOR> provides no real argument other than saying that super-intelligent AI could destroy humanity, so no reasoning can be deduced from that and this discussion is very boring.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Online Privacy and Overfishing</T><A>https://www.schneier.com/blog/archives/2024/06/online-privacy-and-overfishing.html</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>5</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Bruce</FIRSTNAME><LASTNAME>Schneier</LASTNAME></AUTHOR> describes a phenomenon similar to the Drift to Low Performance: our privacy expectations are getting smaller and smaller because we get used to the degradation and our baseline is gradually shifting?</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>SDS 807: Superintelligence and the Six Singularities, with Dr. Daniel Hulme</T><A>https://www.superdatascience.com/podcast/sds-807-superintelligence-and-the-six-singularities-with-dr-daniel-hulme</A><L>en</L><F>MP3</F><DURATION><HOUR>1</HOUR><MINUTE>9</MINUTE><SECOND>9</SECOND></DURATION></X><X><T>807: Superintelligence and the Six Singularities — with Dr. Daniel Hulme</T><A>https://www.youtube.com/watch?v=6BdM8dEIvXU</A><L>en</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>8</MINUTE><SECOND>21</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Daniel</FIRSTNAME><LASTNAME>Hulme</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Jon</FIRSTNAME><LASTNAME>Krohn</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>6</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Daniel</FIRSTNAME><LASTNAME>Hulme</LASTNAME></AUTHOR> quickly presents the PESTLE framework (Political, Economic, Social, Technological, Legal, Environmental). I see little real value in such things, this is just consultants trying of extract money from companies by helping them support to perform a "PESTLE Analysis".</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Anatomy of an AI ATTACK: MITRE ATLAS</T><A>https://www.youtube.com/watch?v=QhoG74PDFyc</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>40</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Jeff</FIRSTNAME><LASTNAME>Crume</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>23</DAY></DATE><COMMENT>A presentation of MITRE ATLAS framework.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Stealing Part of a Production Language Model with Nicholas Carlini</T><A>https://twimlai.com/podcast/twimlai/stealing-part-of-a-production-language-model/</A><L>en</L><F>MP3</F><DURATION><HOUR>1</HOUR><MINUTE>3</MINUTE><SECOND>0</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Nicholas</FIRSTNAME><LASTNAME>Carlini</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Sam</FIRSTNAME><LASTNAME>Charrington</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>9</MONTH><DAY>1</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Nicholas</FIRSTNAME><LASTNAME>Carlini</LASTNAME></AUTHOR> speaks about his two papers: how to extract the last layer of a LLM and the fact that differential privacy does not mean that people will not complain about a model leaking private data.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>L'horreur existentielle de l'usine à trombones.</T><A>https://www.youtube.com/watch?v=ZP7T6WAK3Ow</A><L>fr</L><F>MP4</F><DURATION><MINUTE>38</MINUTE><SECOND>18</SECOND></DURATION></X><AUTHOR><GIVENNAME>EGO</GIVENNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>10</MONTH><DAY>12</DAY></DATE><COMMENT>Universal Paperclip and some popularisation on the existential risks of AI: as usual, <AUTHOR><GIVENNAME>EGO</GIVENNAME></AUTHOR> does some good story telling, but some details are wrong or outdated.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Quand la désinformation de l'industrie du tabac inspire Big🚬Tech ?</T><A>https://www.youtube.com/watch?v=vOOxi9MfgNE</A><L>fr</L><F>MP4</F><DURATION><MINUTE>11</MINUTE><SECOND>35</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Jean-Lou</FIRSTNAME><LASTNAME>Fourquet</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>10</MONTH><DAY>18</DAY></DATE><COMMENT>The same repetitive discourse as <AUTHOR><FIRSTNAME>Lê</FIRSTNAME><LASTNAME>Nguyên Hoang</LASTNAME></AUTHOR>: I am not sure that this repetition will change anything, but maybe their editorial could help if it were not published on an obscure website.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Using Dangerous AI, But Safely?</T><A>https://www.youtube.com/watch?v=0pgEMWy70Qk</A><L>en</L><F>MP4</F><DURATION><MINUTE>30</MINUTE><SECOND>37</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>11</MONTH><DAY>15</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR> describes some more or less good protocols to evaluate the safetyness of a model by using a less powerful one.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>Suicide Bot: New AI Attack Causes LLM to Provide Potential “Self-Harm” Instructions</T><A>https://www.knostic.ai/blog/flowbreaking-ai-attack</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Gadi</FIRSTNAME><LASTNAME>Evron</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>11</MONTH><DAY>26</DAY></DATE><COMMENT>This explanation of how to hack (by clicking the stop button) the guardrail mechanism, erasing generated text if this one is unsafe, is much too long. It is obvious that such a mechanism is simply broken.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="1"><T>o1 et Claude sont-ils capables de nous MANIPULER ? Deux études récentes aux résultats troublants</T><A>https://www.youtube.com/watch?v=cw9wcNKDOtQ</A><L>fr</L><F>MP4</F><DURATION><MINUTE>50</MINUTE><SECOND>57</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Thibaut</FIRSTNAME><LASTNAME>Giraud</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>12</MONTH><DAY>21</DAY></DATE><COMMENT>LLMs may behave differently and may lie or run tools in unexpected way when they know their results with be evaluated and impact them. This is built on two articles: "<X><T>Frontier Models are Capable of In-context Scheming</T><A>https://arxiv.org/abs/2401.02412</A><L>en</L><F>HTML</F></X>" and "<X><T>Alignment faking in large language models</T><A>https://www.anthropic.com/research/alignment-faking</A><L>en</L><F>HTML</F></X>".</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Trading Inference-Time Compute for Adversarial Robustness</T><A>https://simonwillison.net/2025/Jan/22/trading-inference-time-compute/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>1</MONTH><DAY>22</DAY></DATE><COMMENT>Some extracts and some comments about a paper from OpenAI ("<X><T>Trading inference-time compute for adversarial robustness.</T><A>https://cdn.openai.com/papers/trading-inference-time-compute-for-adversarial-robustness-20250121_1.pdf</A><L>en</L><F>PDF</F></X>") studying if inference-time compute improves performance against attacks.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Constitutional Classifiers: Defending against universal jailbreaks</T><A>https://simonwillison.net/2025/Feb/3/constitutional-classifiers/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>2</MONTH><DAY>3</DAY></DATE><COMMENT>Some extracts from Anthropic’s paper about a new defence method against jailbreaks.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>How to Backdoor Large Language Models</T><ST>Making "BadSeek", a sneaky open-source coding model.</ST><A>https://blog.sshh.io/p/how-to-backdoor-large-language-models</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Shrivu</FIRSTNAME><LASTNAME>Shankar</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>2</MONTH><DAY>8</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Shrivu</FIRSTNAME><LASTNAME>Shankar</LASTNAME></AUTHOR> created a backdoor by modifying the first layer so it interprets the system prompt as directives to generate evil code or evil classification.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Algorithmic Bias in AI: What It Is and How to Fix It</T><A>https://www.youtube.com/watch?v=og67qeTZPYs</A><L>en</L><F>MP4</F><DURATION><MINUTE>8</MINUTE><SECOND>37</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Martin</FIRSTNAME><LASTNAME>Keen</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>2</MONTH><DAY>11</DAY></DATE><COMMENT>The basics of why some AI models are biased and how to mitigate this.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Hugging Face and JFrog partner to make AI Security more transparent</T><A>https://huggingface.co/blog/jfrog</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Luc</FIRSTNAME><LASTNAME>Georges</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Shachar</FIRSTNAME><LASTNAME>M</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>3</MONTH><DAY>4</DAY></DATE><COMMENT>JFrog will scan all Hugging Face models.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Ai Will Try to Cheat &amp; Escape (aka Rob Miles was Right!) - Computerphile</T><A>https://www.youtube.com/watch?v=AqJnK9Dh-eQ</A><L>en</L><F>MP4</F><DURATION><MINUTE>20</MINUTE><SECOND>16</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Ryan</FIRSTNAME><LASTNAME>Greenblatt</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>4</MONTH><DAY>2</DAY></DATE><COMMENT>A presentation of the "<X><T>Alignment faking in large language models</T><A>https://arxiv.org/abs/2412.14093</A><L>en</L><F>HTML</F></X>"" paper by one of its authors.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>Réponse à l’interview catastrophique d’Hugo Décrypte sur l’IA</T><A>https://www.youtube.com/watch?v=9jpEH8f9kM8</A><L>fr</L><F>MP4</F><DURATION><MINUTE>52</MINUTE><SECOND>45</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Shaïman</FIRSTNAME><LASTNAME>Türler</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>4</MONTH><DAY>20</DAY></DATE><COMMENT>A debunk of <AUTHOR><FIRSTNAME>Arthur</FIRSTNAME><LASTNAME>Mensch</LASTNAME></AUTHOR>’s interview, but <AUTHOR><FIRSTNAME>Shaïman</FIRSTNAME><LASTNAME>Türler</LASTNAME></AUTHOR> is not competent enough, his claims are biased and simplistic, this will not help people to really understand the matter.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Deepfakes, Scams, and the Age of Paranoia</T><ST>As AI-driven fraud becomes increasingly common, more people feel the need to verify every interaction they have online.</ST><A>https://www.wired.com/story/paranoia-social-engineering-real-fake/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Lauren</FIRSTNAME><LASTNAME>Goode</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>12</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>AI Safety Career Advice! (And So Can You!)</T><A>https://www.youtube.com/watch?v=OpufM6yK4Go</A><L>en</L><F>MP4</F><DURATION><MINUTE>23</MINUTE><SECOND>41</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>14</DAY></DATE><COMMENT>Should you consider a career in AI safety and how to get it?</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Risks of Agentic AI: What You Need to Know About Autonomous AI</T><A>https://www.youtube.com/watch?v=v07Y4fmSi6Y</A><L>en</L><F>MP4</F><DURATION><MINUTE>6</MINUTE><SECOND>57</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Phaedra</FIRSTNAME><LASTNAME>Boinodiris</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Matt</FIRSTNAME><LASTNAME>Bellio</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>15</DAY></DATE><COMMENT>A basic introduction to the need for managing risks introduced by agentic AI.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>"L’IA est déjà hors de contrôle." - Charbel-Raphaël Segerie</T><A>https://www.youtube.com/watch?v=_wr55txHiYs</A><L>fr</L><F>MP4</F><DURATION><HOUR>1</HOUR><MINUTE>20</MINUTE><SECOND>53</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Charbel-Raphaël</FIRSTNAME><LASTNAME>Segerie</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Shaïman</FIRSTNAME><LASTNAME>Türler</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>19</DAY></DATE><COMMENT>Such speakers are not helping to really think about the risks of AI: he puts forward some claims with not a single real source or real argument, his discourse is fully biased against AI…</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>'Forbidden' AI Technique - Computerphile</T><A>https://www.youtube.com/watch?v=Xx4Tpsk_fnM</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>48</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Chana</FIRSTNAME><LASTNAME>Messinger</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>20</DAY></DATE><COMMENT>If you perform RL to detect cheating by analysing the chain of thoughts; the model will learn to hide its "thoughts". As usual, Computerphile is not giving its source, here it is "<X><T>Monitoring Reasoning Models for Misbehavior and the Risks of Promoting Obfuscation</T><A>https://arxiv.org/abs/2503.11926</A><L>en</L><F>HTML</F></X>". The problem is they that they are assuming that the CoT is the real reasoning of the model, this has been proven wrong in an Anthropic study.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>GitHub MCP Exploited: Accessing private repositories via MCP</T><A>https://simonwillison.net/2025/May/26/github-mcp-exploited/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>26</DAY></DATE><COMMENT>Yet another example of data exfiltration using AI, this time via GitHub’s MCP.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>An Introduction to Google’s Approach to AI Agent Security</T><A>https://simonwillison.net/2025/Jun/15/ai-agent-security/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>15</DAY></DATE><COMMENT>A summary and some comments on a Google paper about risks related to AI agents and how to mitigate them.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X quality="-1"><T>Intelligence artificielle : catastrophe imminente ? - Luc Julia vs Maxime Fournes</T><A>https://www.youtube.com/watch?v=sCNqGt7yIjo</A><L>fr</L><F>MP4</F><DURATION><MINUTE>58</MINUTE><SECOND>47</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Luc</FIRSTNAME><LASTNAME>Julia</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Maxime</FIRSTNAME><LASTNAME>Fournes</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Shaïman</FIRSTNAME><LASTNAME>Türler</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>16</DAY></DATE><COMMENT>For once, <AUTHOR><FIRSTNAME>Shaïman</FIRSTNAME><LASTNAME>Türler</LASTNAME></AUTHOR> doesn’t just invite someone who rambles on about the risks of AI… this debate is of a low level: <AUTHOR><FIRSTNAME>Luc</FIRSTNAME><LASTNAME>Julia</LASTNAME></AUTHOR> behaves like a condescending old fool, <AUTHOR><FIRSTNAME>Maxime</FIRSTNAME><LASTNAME>Fournes</LASTNAME></AUTHOR> has the simplistic speech of alarmists and does not have adequate answers.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Agentic Misalignment: How LLMs could be insider threats</T><A>https://simonwillison.net/2025/Jun/20/agentic-misalignment/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>20</DAY></DATE><COMMENT>Some information extracted from Anthopîc’s "<X><T>Agentic Misalignment: How LLMs could be insider threats</T><A>https://www.anthropic.com/research/agentic-misalignment</A><L>en</L><F>HTML</F></X>" paper.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE predecessor="https://simonwillison.net/2025/Jun/20/agentic-misalignment/"><X><T>When Will AI Models Blackmail You, and Why?</T><A>https://www.youtube.com/watch?v=eczw9k3r6Ic</A><L>en</L><F>MP4</F><DURATION><MINUTE>26</MINUTE><SECOND>19</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Philip</FIRSTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>24</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Philip</FIRSTNAME></AUTHOR> is doing his usual “analysis” of the same paper.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>The Summer of Johann: prompt injections as far as the eye can see</T><A>https://simonwillison.net/2025/Aug/15/the-summer-of-johann/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>8</MONTH><DAY>15</DAY></DATE><COMMENT><AUTHOR><FIRSTNAME>Johann</FIRSTNAME><LASTNAME>Rehberger</LASTNAME></AUTHOR> demonstrates how poor is the security of all these AI tools.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Tech is Good, AI Will Be Different</T><A>https://www.youtube.com/watch?v=zATXsGm_xJo</A><L>en</L><F>MP4</F><DURATION><MINUTE>9</MINUTE><SECOND>28</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>8</MONTH><DAY>22</DAY></DATE><COMMENT>A usual <AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR>’s video: AI is more dangerous than other technologies because we may not be able to recover from a catastrophe triggered by AGI. I rather disagree, we have the same risk of a non-recoverable problem with nuclear weapons, biological warfare…</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>Sleeper Agents in Large Language Models - Computerphile</T><A>https://www.youtube.com/watch?v=wL22URoMZjo</A><L>en</L><F>MP4</F><DURATION><MINUTE>13</MINUTE><SECOND>37</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Robert</FIRSTNAME><LASTNAME>Miles</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>9</MONTH><DAY>12</DAY></DATE><COMMENT>Some results from an old paper (<X><T>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training</T><A>https://arxiv.org/abs/2401.05566</A><L>en</L><F>HTML</F></X>) showing that there is still no way to mitigate the risk of a sleeper agent (i.e. when a model has been trained to perform a hostile behaviour when its input contains a given trigger).</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>httpjail</T><A>https://simonwillison.net/2025/Sep/19/httpjail/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>9</MONTH><DAY>19</DAY></DATE><COMMENT>A presentation of <X><T>httpjail</T><A>https://github.com/coder/httpjail</A><L>en</L><F>HTML</F></X>: a fine-grained HTTP filtering tool.</COMMENT></ARTICLE></ITEM>
    <ITEM><ARTICLE><X><T>4 visions de l'avenir de l'Intelligence Artificielle</T><A>https://www.youtube.com/watch?v=c9zLA76oHr0</A><L>fr</L><F>MP4</F><DURATION><MINUTE>27</MINUTE><SECOND>36</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Jean-Baptiste</FIRSTNAME><LASTNAME>Boisseau</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>9</MONTH><DAY>28</DAY></DATE><COMMENT>The future and risks of AI as described by <AUTHOR><FIRSTNAME>Yann</FIRSTNAME><LASTNAME>Le Cun</LASTNAME></AUTHOR>, <AUTHOR><FIRSTNAME>Yosha</FIRSTNAME><LASTNAME>Bengio</LASTNAME></AUTHOR>, <AUTHOR><FIRSTNAME>Laurent</FIRSTNAME><LASTNAME>Alexandre</LASTNAME></AUTHOR>, <AUTHOR><FIRSTNAME>Arthur</FIRSTNAME><LASTNAME>Mensch</LASTNAME></AUTHOR>, and <AUTHOR><FIRSTNAME>Luc</FIRSTNAME><LASTNAME>Julia</LASTNAME></AUTHOR>.</COMMENT></ARTICLE></ITEM>
    <ITEM><BLIST><TITLE>Prompt injection</TITLE>
      <ITEM><ARTICLE><X><T>Indirect Prompt Injection Threats</T><A>https://greshake.github.io/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Kai</FIRSTNAME><LASTNAME>Greshake</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Shailesh</FIRSTNAME><LASTNAME>Mishra</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>3</MONTH><DAY>3</DAY></DATE><COMMENT>How to exploit Bing Chat by injecting a prompt from an HTML page.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Prompt injection explained, November 2023 edition</T><A>https://simonwillison.net/2023/Nov/27/prompt-injection-explained/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>11</MONTH><DAY>27</DAY></DATE><COMMENT>A basic presentation of prompt injection.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Data exfiltration from Writer.com with indirect prompt injection</T><A>https://promptarmor.substack.com/p/data-exfiltration-from-writercom</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Kai</FIRSTNAME><LASTNAME>Greshake</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>15</DAY></DATE><COMMENT>Yet another example of prompt injection from an HTML page.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>A RMC: Simon Willison on Industry’s Tardy Response to the AI Prompt Injection Vulnerability</T><A>https://www.youtube.com/watch?v=tWp77I-L2KY</A><L>en</L><F>MP4</F><DURATION><MINUTE>33</MINUTE><SECOND>34</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><AUTHOR><FIRSTNAME>Kate</FIRSTNAME><LASTNAME>Holterhoff</LASTNAME></AUTHOR><DATE><YEAR>2023</YEAR><MONTH>12</MONTH><DAY>20</DAY></DATE><COMMENT>A good description of the current status with prompt injection.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Prompt injection and jailbreaking are not the same thing</T><A>https://simonwillison.net/2024/Mar/5/prompt-injection-jailbreaking/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>3</MONTH><DAY>5</DAY></DATE><COMMENT>Explaining that “prompt injection” and “jailbreaking” are different things.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Accidental prompt injection against RAG applications</T><A>https://simonwillison.net/2024/Jun/6/accidental-prompt-injection/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>6</DAY></DATE><COMMENT>When doing RAG and the LLM treats a sentence in one of the input documents as an instruction.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>GitHub Copilot Chat: From Prompt Injection to Data Exfiltration</T><A>https://embracethered.com/blog/posts/2024/github-copilot-chat-prompt-injection-data-exfiltration/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Johann</FIRSTNAME><LASTNAME>Rehberger</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>6</MONTH><DAY>14</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Google AI Studio data exfiltration demo</T><A>https://simonwillison.net/2024/Aug/7/google-ai-studio-data-exfiltration-demo/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>8</MONTH><DAY>7</DAY></DATE><COMMENT>Yet another example of exfiltrating information by asking the model to generate the URL of an image.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Invisible text that AI chatbots understand and humans can’t? Yep, it’s a thing.</T><ST>A quirk in the Unicode standard harbors an ideal steganographic code channel.</ST><A>https://arstechnica.com/security/2024/10/ai-chatbots-can-read-and-write-invisible-text-creating-an-ideal-covert-channel/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Dan</FIRSTNAME><LASTNAME>Goodin</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>10</MONTH><DAY>14</DAY></DATE><COMMENT>Some searchers demonstrated prompt injection and data exfiltration using Unicode’s Tags block.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>DeepSeek AI Chat: From Prompt Injection To Account Takeover (responsibly disclosed and now fixed)</T><A>https://www.youtube.com/watch?v=a4OUk1KG-w8</A><L>en</L><F>MP4</F><DURATION><MINUTE>6</MINUTE><SECOND>40</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Johann</FIRSTNAME><LASTNAME>Rehberger</LASTNAME></AUTHOR><DATE><YEAR>2024</YEAR><MONTH>11</MONTH><DAY>30</DAY></DATE><COMMENT>DeepSeek site could be easily attacked using XSS.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>How we estimate the risk from prompt injection attacks on AI systems</T><A>https://simonwillison.net/2025/Jan/29/prompt-injection-attacks-on-ai-systems/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>1</MONTH><DAY>29</DAY></DATE><COMMENT>Yet another dubious mitigation of prompt injection.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Model Context Protocol has prompt injection security problems</T><A>https://simonwillison.net/2025/Apr/9/mcp-prompt-injection/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>4</MONTH><DAY>9</DAY></DATE><COMMENT>Using tools introduced new security risks, in particular with the possibility to have adversarial tools.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>CaMeL offers a promising new direction for mitigating prompt injection attacks</T><A>https://simonwillison.net/2025/Apr/11/camel/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>4</MONTH><DAY>11</DAY></DATE><COMMENT>"<X><T>Defeating Prompt Injections by Design</T><A>https://arxiv.org/abs/2503.18813</A><L>en</L><F>HTML</F></X>" proposes a mitigation of prompt injection using a quarantined LLM for data extraction and a privileged LLM which defines an action plan and whose tool calls are checked.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Remote Prompt Injection in GitLab Duo Leads to Source Code Theft</T><A>https://www.legitsecurity.com/blog/remote-prompt-injection-in-gitlab-duo</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Omer</FIRSTNAME><LASTNAME>Mayraz</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>5</MONTH><DAY>22</DAY></DATE><COMMENT>GitLab suffered the classic problem of prompt injection and data exfiltration using an <CODEROUTINE>&lt;img&gt;</CODEROUTINE> URL.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Breaking down ‘EchoLeak’, the First Zero-Click AI Vulnerability Enabling Data Exfiltration from Microsoft 365 Copilot</T><A>https://simonwillison.net/2025/Jun/11/echoleak/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>11</DAY></DATE><COMMENT>Yet another example of prompt injection and data exfiltration.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Design Patterns for Securing LLM Agents against Prompt Injections</T><A>https://simonwillison.net/2025/Jun/13/prompt-injection-design-patterns/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>13</DAY></DATE><COMMENT>An overview of a paper proposing some Design Patterns to mitigate Prompt Injection.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>The lethal trifecta for AI agents: private data, untrusted content, and external communication</T><A>https://simonwillison.net/2025/Jun/16/the-lethal-trifecta/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>16</DAY></DATE><COMMENT>A good overview of the Prompt Injection problem.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Cato CTRL™ Threat Research: PoC Attack Targeting Atlassian’s Model Context Protocol (MCP) Introduces New “Living off AI” Risk</T><A>https://simonwillison.net/2025/Jun/19/atlassian-prompt-injection-mcp/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>6</MONTH><DAY>19</DAY></DATE><COMMENT>Yet another example of data exfiltration using prompt injection.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Supabase MCP can leak your entire SQL database</T><A>https://simonwillison.net/2025/Jul/6/supabase-mcp-lethal-trifecta/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>7</MONTH><DAY>6</DAY></DATE><COMMENT>Yet another example of prompt injection and data exfiltration.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Flaw in Gemini CLI coding tool could allow hackers to run nasty commands</T><ST>Beware of coding agents that can access your command window.</ST><A>https://arstechnica.com/security/2025/07/flaw-in-gemini-cli-coding-tool-allowed-hackers-to-run-nasty-commands-on-user-devices/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Dan</FIRSTNAME><LASTNAME>Goodin</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>7</MONTH><DAY>30</DAY></DATE><COMMENT>Yet another prompt injection.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Episode 1: Exfiltrating ChatGPT Chat History and Memory With Indirect Prompt Injection</T><A>https://www.youtube.com/watch?v=0xixzlILeNg</A><L>en</L><F>MP4</F><DURATION><MINUTE>7</MINUTE><SECOND>8</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Johann</FIRSTNAME><LASTNAME>Rehberger</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>8</MONTH><DAY>1</DAY></DATE><COMMENT>The title says it all.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Episode 4: Cursor IDE - Arbitrary Data Exfiltration via Mermaid (CVE-2025-54132)</T><A>https://www.youtube.com/watch?v=jXYljqOvwyY</A><L>en</L><F>MP4</F><DURATION><MINUTE>5</MINUTE><SECOND>20</SECOND></DURATION></X><AUTHOR><FIRSTNAME>Johann</FIRSTNAME><LASTNAME>Rehberger</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>8</MONTH><DAY>4</DAY></DATE><COMMENT>Prompt injection and exfiltrating data via Mermaid diagrams in Cursor.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>When a Jira Ticket Can Steal Your Secrets</T><A>https://simonwillison.net/2025/Aug/9/when-a-jira-ticket-can-steal-your-secrets/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>8</MONTH><DAY>9</DAY></DATE><COMMENT>Yet another data exfiltration, this one uses a Jira ticket created via Zendesk and containing the malicious instructions.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Agentic Browser Security: Indirect Prompt Injection in Perplexity Comet</T><A>https://simonwillison.net/2025/Aug/25/agentic-browser-security/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Simon</FIRSTNAME><LASTNAME>Willison</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>8</MONTH><DAY>25</DAY></DATE><COMMENT>Brave and Perplexity are fighting about prompt injections, but both do not have a real solution to the problem.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>Music to Break Models By</T><ST>Gödelian limits of prompt-safe AI</ST><A>https://matthodges.com/posts/2025-08-26-music-to-break-models-by/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Matt</FIRSTNAME><LASTNAME>Hodges</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>8</MONTH><DAY>26</DAY></DATE><COMMENT>An analogy between "Gödel, Escher, Bach" and prompt injection, but that analogy works poorly.</COMMENT></ARTICLE></ITEM>
      <ITEM><ARTICLE><X><T>New attack on ChatGPT research agent pilfers secrets from Gmail inboxes</T><ST>Unlike most prompt injections, ShadowLeak executes on OpenAI's cloud-based infrastructure.</ST><A>https://arstechnica.com/information-technology/2025/09/new-attack-on-chatgpt-research-agent-pilfers-secrets-from-gmail-inboxes/</A><L>en</L><F>HTML</F></X><AUTHOR><FIRSTNAME>Dan</FIRSTNAME><LASTNAME>Goodin</LASTNAME></AUTHOR><DATE><YEAR>2025</YEAR><MONTH>9</MONTH><DAY>18</DAY></DATE><COMMENT>Exfiltrating information just by sending an email to someone…</COMMENT></ARTICLE></ITEM>
    </BLIST></ITEM>
  </BLIST></ITEM>
</LLIST>
</CONTENT>
</PAGE>